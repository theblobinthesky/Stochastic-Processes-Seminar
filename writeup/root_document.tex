\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{dsfont, mathtools, amsthm, diagbox}

\title{Introduction to Entropy}
\author{Erik Stern}

\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\paren}[1]{\left(#1\right)}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\begin{document}
\maketitle

\begin{abstract}
Your abstract.
\end{abstract}

\tableofcontents

\section{Entropy and Mutual information}
\subsection{Definitions and Conventions}

Let $(\mathcal{X}, \mathcal{A}, \mathds{P_\mathcal{A}})$ be a probability space. 
Let $X: \mathcal{X} \rightarrow \mathbb{R}$ be a discrete random variable on the space with probability density function $f_X: \mathcal{X} \rightarrow \mathbb{R}_+$. We use the shorthand notation $p(x) = \mathds{P_\mathcal{A}}[X = x]$. Let $(\mathcal{Y}, \mathcal{B}, \mathds{P_\mathcal{B}})$ be a probability space. 
Let $Y: \mathcal{Y} \rightarrow \mathbb{R}$ be a discrete random variable on the space with probability density function $f_Y: \mathcal{Y} \rightarrow \mathbb{R}_+$. We use the shorthand notation $p(y) = \mathds{P_\mathcal{B}}[X = x]$.

This is a test.

\begin{definition}
    TODO: This is all incorrectly defined.
    Let $X$ be a discrete random variable with distribution $p(x)$. \\
    We define entropy as $H_q(X) = \mathbb{E}\paren{-\log_q p(X)}$.
    Since entropy was originally defined in the context of compression by Shannon in TODO,
    We usually use $q = 2$ and $H(X) = \mathbb{E}\paren{-\log_2 p(X)}$.
    TODO: Add $log = log_2$ convention.
\end{definition}

\begin{theorem}
Existence of Entropy: If $\mathcal{X}$ is finite, $H_q(X)$ exists.

Example, when Entropy does not exist:
Let $X = $,
\end{theorem}
\begin{proof}
\end{proof}

Convention: $0\log(0) = 0$.

Using definition of expected value: $H_p(X) = \mathbb{E}\paren{-\log_q(p(X)) = \mathbb{E}\paren{\frac{1}{\log_q(p(X))}}}$.

\begin{definition}\label{def:entropy-mutual-info-def}
    Let $X, Y$ be discrete random variables with marginal distributions $p(x), p(y)$ and with joint distribution $p(x, y)$.
    We define the following:
    \begin{align*}
        \text{Conditional Entropy:} & \quad H(X \mid Y) = -\mathbb{E}\paren{\log p(X \mid Y)} \\
        \text{Joint Entropy:}       & \quad H(X, Y) = -\mathbb{E}\paren{\log p(X, Y)} \\
        \text{Relative Entropy:}    & \quad D(p(x) \| q(x)) = \mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}} \text{with the conventions} ... \\
        \text{Mutual information:}  & \quad I(X; Y) \defeq D(p(x, y) \| p(x)p(y)) \\
        \text{Conditional Mutual information:}  & \quad I(X; Y \mid Z) \defeq H(X \mid Z) - H(X \mid Y, Z)
    \end{align*}
\end{definition}

\begin{remark}
    From now on, we always define the two discrete random variables $X, Y$ from Definition~\ref{def:entropy-mutual-info-def}.
\end{remark}

\begin{remark}
    Under the assumptions of Definition~\ref{def:entropy-mutual-info-def}, we have
    \begin{align*}
        D(p(x) \| q(x)) &= \mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}}                   && \text{(def.~of relative entropy)} \\
                        &= \sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}}        && \text{(def.~of expected value)}
    \end{align*}
    To understand the conventions, we can look at the limit cases:
    \begin{enumerate}
        \item \textbf{Case $p \in (0, 1], q = 0$:} $\lim_{q \to 0^+} p \log \frac{p}{q} = \lim_{q \to 0^+} (p \log p - p \log q) = \infty$.
        \item \textbf{Case $p = 0, q \in (0, 1]$:} $0 \log \frac{0}{q} = 0$.
        \item \textbf{Case $p = q = 0$:} Case 1 logic yields $\lim_{q \to 0^+} p \log \frac{p}{q} = \infty$ and Case 2 logic yields $0 \log \frac{0}{0} = 0$. 
    \end{enumerate}
    As we want $\sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}}$ to sum over $x \in \mathcal{X}, p(x) > 0$, we choose the convention $0 \log \frac{0}{0} = 0$.
    We can visualize the the pointwise relative entropy function $(p, q) \mapsto \log \frac{p}{q}$:
    \begin{center}
        \includegraphics[width=0.6\linewidth]{../plots/pointwise_rel_entropy_continuation.pdf}
    \end{center}
    We can calculate the relative entropies for an example. The more alike the distributions are, the closer to zero the relative entropy is.
    Let $X \sim B(20, \alpha)$ and $Y \sim B(20, \beta)$ with $\paren{\alpha, \beta} \in [0, 1]^2$.
    \begin{align*}
                                 & \quad D(p(x) \| q(x)) = \sum_{x = 0}^{20}{p(x) \log \frac{p(x)}{q(x)}} \\
        \alpha=0, \beta=1:       & \quad D(p(x) \| q(x)) = 0 \log \frac{0}{1} + 1 \log \frac{1}{0} = 0 + \infty = \infty \\
        \alpha=0.1, \beta=0.9:   & \quad D(p(x) \| q(x)) \approx 50.7 \\
        \alpha=0.3, \beta=0.7:   & \quad D(p(x) \| q(x)) \approx 9.8 \\
        \alpha= 0.5, \beta= 0.5: & \quad D(p(x) \| q(x)) = \sum_{x = 0}^{20}{p(x) \log 1} = 0 \\
    \end{align*}
    \begin{center}
        \includegraphics[width=1.0\linewidth]{../plots/relative_entropy_binomial_distributions.pdf}
    \end{center}
\end{remark}

\newpage
\subsection{Chain Rules and Mutual Information}
\begin{remark}
    The relationship between entropy, conditional entropy and mutual information can be visualized:
    \begin{center}
        \includegraphics[width=1.0\linewidth]{../plots/entropy_mutual_info_venn_diagram.pdf}
    \end{center}
\end{remark}

\begin{theorem}
    The following statements about Entropy and Mutual Information are called Chain Rules:
    \begin{enumerate}
        \item
        \item
    \end{enumerate}
\end{theorem}
\begin{proof}
\end{proof}

\begin{corollary}
\end{corollary}
\begin{proof}
\end{proof}

\begin{corollary}
    The following statements are specialized for two random variables $X, Y$:
    \begin{enumerate}
        \item $H(X, Y) = H(X) + H(Y \mid X)$
        \item
    \end{enumerate}
\end{corollary}
\begin{proof}
    \begin{align*}
        H(X, Y) &= -\mathbb{E}\paren{\log p(X, Y)}                                          && \text{(def. entropy)}        \\
                &= -\mathbb{E}\paren{\log p(X)p(Y \mid X)}                                  && \text{(conditional prob.)}   \\
                &= -\mathbb{E}\paren{\log p(X)} + -\mathbb{E}\paren{\log p(Y \mid X)}       && \text{(log sum property)}    \\
                &= H(X) + H(Y \mid X)                                                       && \text{(def. entropy)}        \\
    \end{align*}
\end{proof}

\begin{theorem}
    There are multiple equivalent ways to express Mutual Information:
    \begin{enumerate}
        \item $I(X; Y) = H(Y) - H(Y | X)$
        \item $I(X; Y) = I(Y; X)$
        \item $I(Y; X) = H(X) - H(X | Y)$
        \item $I(X; Y) = H(X, Y)$
        \item $I(X; X) = H(X)$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item We can use the definition of mutual information and relative entropy to obtain:
            \begin{align*}
                I(X; Y) &= D(p(x, y) \| p(x)p(y))                                                           && \text{(by def.~of mutual info.)}\\
                        &= \mathbb{E}_{p(x, y)}\paren{\log \frac{p(x, y)}{p(x)p(y)}}                        && \text{(by def.~relative entropy)} \\
                        &= \mathbb{E}_{p(x, y)}\paren{\log \frac{p(x)p(y|x)}{p(x)p(y)}}                     && \text{(using cond.~probability)} \\
                        &= \mathbb{E}_{p(x, y)}\paren{\log \frac{p(y|x)}{p(y)}}                             && \text{(simplify fraction)} \\
                        &= \mathbb{E}_{p(x, y)}\paren{\log p(y|x)} - \mathbb{E}_p(x, y)\paren{\log p(y)}    && \text{(simplify logarithm)} \\
                        &= H(Y | X) - H(Y)                                                                  && \text{(by def.~of entropy)}
            \end{align*}
        \item The definition of mutual information yields:
            \begin{align*}
                I(X; Y) &= D(p(x, y) \| p(x)p(y))       && \text{(by def.~of mutual info.)} \\
                        &= D(p(y, x) \| p(y)p(x))       && \text{(TODO: Idk)}\\
                        &= I(X; Y)
            \end{align*}
        \item Follows directly from 2 and 3.
        \item \begin{align*}
                I(X; Y) &= H(Y) - H(X | Y)                  && \text{(by 1)} \\
                        &= H(Y) - \paren{H(X, Y) - H(X)}    && \text{(chain rule)} \\
                        &= H(X) + H(Y) - H(X, Y)
            \end{align*}
        \item Using 1 we get $I(X; X) = H(X) - H(X | X) = H(X)$.
    \end{enumerate}
\end{proof}



% TODO: Relative entropy is often used as a distance, but not actually a metric.


\newpage
\section{Inequalities for Entropy and Mutual Information}
\subsection{Convexity and Jensen Inequality}

\begin{remark}
    We use the common definition of convex functions, concave functions from analysis.
\end{remark}

% TODO: Make sure which theorems and definitions assume discrete.

\begin{theorem}
    Let $X$ be a random variable (not necessarily discrete) and $f$ a function. % TODO: Maybe we just make it discrete. Seems like the easier option!
    \begin{enumerate}
        \item If $f$ is convex, we have $\mathbb{E} f(X) \ge f(\mathbb{E} X)$.
        \item If $f$ is concave, we have $\mathbb{E} f(X) \le f(\mathbb{E} X)$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item
            % Let $n \in \mathbb{N}, n \gt 2$ and $p_1, \cdots, p_n \in $ and $x_1, \cdots, x_n$.
            Distinguish two cases. If $\mathcal{X}$ is finite: \\
            We show $f\paren{\sum_{i=1}^{n}{p_i x_i}} \le \sum_{i=1}^{n}{p_i f(x_i)}$ by induction. \\
            The definition of convexity yields the base case $i = 2$: $f(p_1 x_1 + p_2 x_2) \le p_1 f(x_1) + p_2 f(x_2)$. \\
            We assume the claim holds for $n - 1$ and the induction case goes as follows: \\
            \begin{align*}
                f\paren{\sum_{i=1}^{n}{p_i x_i}} &= f\paren{p_1 x_1 + (1 - p_1) \sum_{i=2}^{n}{\frac{p_i}{1 - p_1} x_i}}                                                                        \\
                &\le p_1 f\paren{x_1} + (1 - p_1) f\paren{\sum_{i=2}^{n}{\frac{p_i}{1 - p_1} x_i}}           && \text{(def. of convexity)}                                                      \\
                &\le p_1 f\paren{x_1} + (1 - p_1) \sum_{i=2}^{n}{\frac{p_i}{1 - p_1} f\paren{{x_i}}}         
                && \text{(induct. hypo. applies bc. of $\sum_{i = 2}^{n}{\frac{p_i}{1 - p_1}}$ = 1)} \\
                &= \sum_{i=1}^{n}{p_i f\paren{{x_i}}}
            \end{align*}
            Else: \\% If $\mathcal{X}$ is countably infinite, the claim follows using ... .
            % TODO: Case missing?
        \item
            Follows from part 1 applied to $-f$.
    \end{enumerate}
\end{proof}

\begin{corollary}
    Entropy and Mutual Information are non-negative:
    \begin{enumerate}
        \item $0 \le H(X)$
        \item $0 \le D(p(x) \| q(x))$
        \item $0 \le I(X; Y)$
    \end{enumerate}
\end{corollary}
\begin{proof}
    \begin{enumerate}
        \item 
            Note that $\log(\frac{1}{[0, 1]}) = \log([1, \infty]) = [0, \infty]$ 
            and $p(X)(\mathcal{X}) \in [0, 1]$. \\
            Using the monotonicity of the expected value, we obtain
            \begin{equation*}
                0 \le \mathbb{E}\paren{\log \paren{\frac{1}{p(X)}}}
                = -\mathbb{E}\paren{\log \paren{p(X)}} = H(X)
            \end{equation*}

        \item 
            We can prove this using Jensens inequality on a \textit{concave} function: % TODO: Make sure we use the support correctly.
            \begin{align*}
                -D(p(x) \| q(x)) &= -\mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}}         && \text{(def. of relative entropy)}            \\
                &\le -\log\paren{\mathbb{E}_p \frac{p(X)}{q(X)}}                        
                = \log\paren{\mathbb{E}_p \frac{q(X)}{p(X)}}                            && \text{(-log is convex, property of log)}     \\
                &= \log\paren{\sum_{x \in \mathcal{X}}{p(x) \frac{q(x)}{p(x)}}}         
                = \log\paren{\sum_{x \in \mathcal{X}}{q(x)}}                            && \text{(def. of exp. value, simplify expr.)}  \\
                &= \log\paren{1} = 0                                                    && \text{(q is a prob. function)}               \\
            \end{align*}
            So equivalently, we have $D(p(x) \| q(x)) \ge 0$.
        \item Follows from part 1: $I(X; Y) = D(p(x, y) \| p(x)p(y)) \ge 0$.
    \end{enumerate}
\end{proof}

\begin{remark}
    There are multiple natural questions we can ask about Entropy.
    We will look at an example for each of them and then prove the results in the follow-up theorem.
    \begin{enumerate}
        \item What distribution maximises the value of Entropy?
            I simulated 

            From the data, it seems plausible that the uniform distribution maximises entropy. We
            % TODO: How difficult is it to sample from the simplex?
        \item Can joint entropy increase if we add redundant information? \\
            Let $\Omega = \{1, 2\}$, p uniform TODO, $X, Y: \Omega \rightarrow \mathbb{R}$, $X(\omega) = \omega$ and $Y(\omega) = 2\omega$.
            $Y$ is redundant to $X$, as $Y = 2X$. \\
            % We can now calculate the entropy to verify the intuition, that adding $Y
            \begin{align*}
                H(X) &= \sum_{i = 1}^{2}{-0.5 \log 0.5} = -\log 0.5 = \log 2 = 1 \\
                H(X, Y) &= \sum_{(i, j) \in \{(1, 2), (2, 4)\}}{-0.25 \log 0.25} = 2 * 0.25 * 2 = 1
                % TODO: These sums could maybe be expressed more formally first and then substituted in
            \end{align*}
            Pairs like $(1, 4)$ have probability zero and do not contribute to the sum in $H(X, Y)$.
            This example suggest entropy never increases. We will prove that in the follow-up theorem.
        \item What happens to the Entropy if we add independent noise to our measurements? \\
            Let $X \sim U(\{1, 2, 3, 4\})$ be the original signal, $N \sim B(3, 0.5)$ the noise and let $X, N$ be independent. 
            Then $S \defeq X + N$ is a noisy signal. \\

            We can compute the crirical variance $a$.
            % TODO
    \end{enumerate}
\end{remark}

\begin{theorem}
    We can formalise the previous observations including some more:
    \begin{enumerate}
        \item More information can only \textit{decrease} entropy: $H(X \mid Y) \le H(X)$.
        \item The uniform distribution maximizes entropy: \\
            $H(X) \le \log |\mathcal{X}|$ and $H(X) = \log |\mathcal{X}| \iff X \sim U(\mathcal{X})$.
        \item If information from $Y$ does not add anything to $X$, then $Y$ must be derived from $X$: \\ % TODO: Dis true backwards?
            $H(Y \mid X) = 0 \implies \exists f\vcentcolon Y = f(X) \; \text{almost surely}$
        \item If independent noise is added to a random variable, entropy can only increase: \\
            Set $Z = X + Y$. Then we have $X, Y \text{independent} \implies H(X) \le H(Z) \land H(Y) \le H(Z)$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \newcommand{\injoint}{\in \mathcal{X} \times \mathcal{Y}}

    \begin{enumerate}
        \item $0 \le I(X; Y) = H(X) - H(X \mid Y) \iff H(X \mid Y) \le H(X)$
        \item
            Let $Y \sim U(\mathcal{X})$ st. $\forall x \in \mathcal{X}\vcentcolon q(x)=\frac{1}{|\mathcal{X}|}$.
            \begin{align*}
                0 &\le D(p(x) \| q(x))
                = \mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}}
                = \sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}} \\
                &= \sum_{x \in \mathcal{X}}{p(x) \log\paren{p(x)|\mathcal{X}|}}
                = \sum_{x \in \mathcal{X}}{p(x) \log p(x)} + |\mathcal{X}| \sum_{x \in \mathcal{X}}{p(x)}\\
                &= -H(X) + |\mathcal{X}| = |\mathcal{X}| - H(X)
            \end{align*}
            This is equivalent to $H(X) \le \log |\mathcal{X}|$.
            TODO: Iff part.
        \item
            We have $H(Y \mid X) = -\mathbb{E}\paren{\log p(Y \mid X)} = \sum_{(x, y) \injoint}{-p(x, y) \log p(y \mid x)}$. \\
            Additionally, we have $\forall (x, y) \in \mathcal{X} \times \mathcal{Y}: -p(x, y) \log p(y \mid x) \ge 0$. \\
            Combining those facts, we get
            \begin{align*}
                &H(Y \mid X) = 0 \\
                \iff &\forall (x, y) \injoint: p(x, y) \log p(y \mid x) = 0 \\
                \iff &\forall (x, y) \injoint: p(x, y) = 0 \oplus \log p(y \mid x) = 0 \\
                \iff &\forall (x, y) \injoint: p(x, y) = 0 \oplus p(y \mid x) = 1
            \end{align*}
            This tells us, that either $p(x, y) = 0$ or $p(x, y) = p(y \mid x)p(x) = p(x) = 1$. \\
            Now we can finish up the argument. Set $A = \{x \in \mathcal{X}: p(x) > 0\}$. \\
            Define $(y_x)_{x \in \mathcal{X}}$ such that $\forall x \in \mathcal{X}: p(x, y_x) > 0$. \\
            Set $f: A \rightarrow \mathcal{Y}, x \mapsto y_x$. This gets us $Im f = {y_x: x \in \mathcal{X}} = {y \in \mathcal{Y}: p(x, y) > 0}$. \\
            So $Y = f(X) \; \text{almost surely}$.
        \item
            We have
            \begin{align*}
                 H(Z \mid X) &= -\mathbb{E}\paren{\log p(Z \mid X)} = \sum_{(x, y) \injoint}{-p(x, y) \log P[Z = x + y \mid X = x]} \\
                 &= \sum_{(x, y) \injoint}{-p(x, y) \log P[X + Y = x + y \mid X = x]} \\
                 &= \sum_{(x, y) \injoint}{-p(x, y) \log P[Y = y \mid X = x]} \\
                 &= -\mathbb{E}\paren{\log p(Y \mid X)} = H(Y \mid X)
            \end{align*}
            $H(Z, X) = H(X) + H(Z \mid X)$
            $H(Y, X) = H(X) + H(Y \mid X)$
            % TODO: Fix this shit!

            $H(Y, X) = H(X) + H(Y \mid X)$
            Using the Chain rule, we get $H(X, Y) = H(Y \mid X) + H(X)$
    \end{enumerate}
\end{proof}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.50\linewidth]{../plots/entropy_function.png}
%     \caption{\label{fig:entropy_function}The entropy function is a concave function.}
% \end{figure}

% TODO: Add deep learning example.

\newpage
\subsection{The Information Inequality}
%         \item $p \mapsto H(p)$ is concave.

Hello

\newpage
\subsection{Application to Optimisation}

Hello


\bibliographystyle{alpha}
\bibliography{sample}

% TODO: \input{chapters/ch1_einleitung.tex}
% TODO: \FloatBarrier

\end{document}
