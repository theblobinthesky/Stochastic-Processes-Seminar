\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{dsfont, mathtools, amsthm}

\title{Your Paper}
\author{You}

\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\paren}[1]{\left(#1\right)}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\begin{document}
\maketitle

\begin{abstract}
Your abstract.
\end{abstract}

\section{Entropy and Mutual information}

Let $(\mathcal{X}, \mathcal{A}, \mathds{P_\mathcal{A}})$ be a probability space. 
Let $X: \mathcal{X} \rightarrow \mathbb{R}$ be a discrete random variable on the space with probability density function $f_X: \mathcal{X} \rightarrow \mathbb{R}_+$. We use the shorthand notation $p(x) = \mathds{P_\mathcal{A}}[X = x]$. Let $(\mathcal{Y}, \mathcal{B}, \mathds{P_\mathcal{B}})$ be a probability space. 
Let $Y: \mathcal{Y} \rightarrow \mathbb{R}$ be a discrete random variable on the space with probability density function $f_Y: \mathcal{Y} \rightarrow \mathbb{R}_+$. We use the shorthand notation $p(y) = \mathds{P_\mathcal{B}}[X = x]$.

This is a test.

\begin{definition}
    Let $X$ be a discrete random variable with distribution $p(x)$. \\
    We define entropy as $H_q(X) = \mathbb{E}\paren{-\log_q p(X)}$.
    Since entropy was originally defined in the context of compression by Shannon in TODO,
    we usually use $q = 2$ and $H(X) = \mathbb{E}\paren{-\log_2 p(X)}$.
\end{definition}

\begin{theorem}
Existence of Entropy: If $\mathcal{X}$ is finite, $H_q(X)$ exists.

Example, when Entropy does not exist:
Let $X = $,
\end{theorem}
\begin{proof}
\end{proof}

Convention: $0\log(0) = 0$.

Using definition of expected value: $H_p(X) = \mathbb{E}\paren{-\log_q(p(X)) = \mathbb{E}\paren{\frac{1}{\log_q(p(X))}}}$.

Properties of Entropy:
\begin{definition}\label{def:entropy-mutual-info-def}
    Let $X, Y$ be discrete random variables with marginal distributions $p(x), p(y)$ and with joint distribution $p(x, y)$.
    We define the following:
    \begin{align*}
        \text{Conditional Entropy:} & \quad H(X \mid Y) = -\mathbb{E}\paren{\log p(X \mid Y)} \\
        \text{Joint Entropy:}       & \quad H(X, Y) = -\mathbb{E}\paren{\log p(X, Y)} \\
        \text{Relative Entropy:}    & \quad D(p(x) \| q(x)) = \mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}} \text{with the conventions} ... \\
        \text{Mutual information:}  & \quad I(X; Y) \defeq D(p(x, y) \| p(x)p(y))
    \end{align*}
\end{definition}

\begin{remark}
    From now on, we always define the two discrete random variables $X, Y$ from Definition~\ref{def:entropy-mutual-info-def}.
\end{remark}

\newpage
\begin{remark}
    Under the assumptions of Definition~\ref{def:entropy-mutual-info-def}, we have
    \begin{align*}
        D(p(x) \| q(x)) &= \mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}}                   && \text{(def.~of relative entropy)} \\
                        &= \sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}}        && \text{(def.~of expected value)}
    \end{align*}
    To understand the conventions, we can look at the limit cases:
    \begin{enumerate}
        \item \textbf{Case $p \in (0, 1], q = 0$:} $\lim_{q \to 0^+} p \log \frac{p}{q} = \lim_{q \to 0^+} (p \log p - p \log q) = \infty$.
        \item \textbf{Case $p = 0, q \in (0, 1]$:} $0 \log \frac{0}{q} = 0$.
        \item \textbf{Case $p = q = 0$:} Case 1 logic yields $\lim_{q \to 0^+} p \log \frac{p}{q} = \infty$ and Case 2 logic yields $0 \log \frac{0}{0} = 0$. 
    \end{enumerate}
    As we want $\sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}}$ to sum over $x \in \mathcal{X}, p(x) > 0$, we choose the convention $0 \log \frac{0}{0} = 0$.
    We can visualize the the pointwise relative entropy function $(p, q) \mapsto \log \frac{p}{q}$:
    \begin{center}
        \includegraphics[width=0.6\linewidth]{../plots/pointwise_rel_entropy_continuation.pdf}
    \end{center}
    We can calculate the relative entropies for an example. The more alike the distributions are, the closer to zero the relative entropy is.
    Let $X \sim B(20, \alpha)$ and $Y \sim B(20, \beta)$ with $\paren{\alpha, \beta} \in [0, 1]^2$.
    \begin{center}
        \includegraphics[width=1.0\linewidth]{../plots/relative_entropy_binomial_distributions.pdf}
    \end{center}
    \begin{align*}
        D(p(x) \| q(x)) &= \sum_{x = 0}^{20}{p(x) \log \frac{p(x)}{q(x)}}                                                             \\
        D(p(x) \| q(x)) &= 0 \log \frac{0}{1} + 1 \log \frac{1}{0} = 0 + \infty = \infty                && (\alpha=0, \beta=1)        \\
        D(p(x) \| q(x)) &\approx 35.2                                                                   && (\alpha=0.1, \beta=0.9)    \\
        D(p(x) \| q(x)) &\approx 6.8                                                                    && (\alpha=0.3, \beta=0.7)    \\
        D(p(x) \| q(x)) &= \sum_{x = 0}^{20}{p(x) \log 1} = 0                                           && (\alpha= 0.5, \beta= 0.5)
    \end{align*}
\end{remark}

\newpage
How do these concepts relate?

TODO: Add a venn diagram visualisation.

\begin{theorem}
    The chain rule holds:
    \begin{align*}
        H(X, Y) = H(X) + H(Y \mid X)
    \end{align*}
\end{theorem}
\begin{proof}
    \begin{align*}
        H(X, Y) &= -\mathbb{E}\paren{\log p(X, Y)}                                          && \text{(def. entropy)}        \\
                &= -\mathbb{E}\paren{\log p(X)p(Y \mid X)}                                  && \text{(conditional prob.)}   \\
                &= -\mathbb{E}\paren{\log p(X)} + -\mathbb{E}\paren{\log p(Y \mid X)}       && \text{(log sum property)}    \\
                &= H(X) + H(Y \mid X)                                                       && \text{(def. entropy)}        \\
    \end{align*}
\end{proof}

\newpage
\begin{remark}
    The relationship between entropy, conditional entropy and mutual information can be visualized:
    \begin{center}
        \includegraphics[width=1.0\linewidth]{../plots/entropy_mutual_info_venn_diagram.pdf}
    \end{center}
\end{remark}

\begin{theorem}
    We can formalise the visual insight from before:
    \begin{enumerate}
        \item $I(X; Y) = H(Y) - H(Y | X)$
        \item $I(X; Y) = I(Y; X)$
        \item $I(Y; X) = H(X) - H(X | Y)$
        \item $I(X; Y) = H(X, Y)$
        \item $I(X; X) = H(X)$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item We can use the definition of mutual information and relative entropy to obtain:
            \begin{align*}
                I(X; Y) &= D(p(x, y) \| p(x)p(y))                                                           && \text{(by def.~of mutual info.)}\\
                        &= \mathbb{E}_{p(x, y)}\paren{\log \frac{p(x, y)}{p(x)p(y)}}                        && \text{(by def.~relative entropy)} \\
                        &= \mathbb{E}_{p(x, y)}\paren{\log \frac{p(x)p(y|x)}{p(x)p(y)}}                     && \text{(using cond.~probability)} \\
                        &= \mathbb{E}_{p(x, y)}\paren{\log \frac{p(y|x)}{p(y)}}                             && \text{(simplify fraction)} \\
                        &= \mathbb{E}_{p(x, y)}\paren{\log p(y|x)} - \mathbb{E}_p(x, y)\paren{\log p(y)}    && \text{(simplify logarithm)} \\
                        &= H(Y | X) - H(Y)                                                                  && \text{(by def.~of entropy)}
            \end{align*}
        \item The definition of mutual information yields:
            \begin{align*}
                I(X; Y) &= D(p(x, y) \| p(x)p(y))       && \text{(by def.~of mutual info.)} \\
                        &= D(p(y, x) \| p(y)p(x))       && \text{(TODO: Idk)}\\
                        &= I(X; Y)
            \end{align*}
        \item Follows directly from 2 and 3.
        \item \begin{align*}
                I(X; Y) &= H(Y) - H(X | Y)                  && \text{(by 1)} \\
                        &= H(Y) - \paren{H(X, Y) - H(X)}    && \text{(chain rule)} \\
                        &= H(X) + H(Y) - H(X, Y)
            \end{align*}
        \item Using 1 we get $I(X; X) = H(X) - H(X | X) = H(X)$.
    \end{enumerate}
\end{proof}



% TODO: Relative entropy is often used as a distance, but not actually a metric.


\newpage
\section{Properties of Entropy and Mutual Information}

\begin{remark}
    We use the common definition of convex functions, concave functions from analysis.
\end{remark}

% TODO: Make sure which theorems and definitions assume discrete.

\begin{theorem}
    Let $X$ be a random variable (not necessarily discrete) and $f$ a function.
    \begin{enumerate}
        \item If $f$ is convex, we have $\mathbb{E} f(X) \ge f(\mathbb{E} X)$.
        \item If $f$ is concave, we have $\mathbb{E} f(X) \le f(\mathbb{E} X)$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item
            % Let $n \in \mathbb{N}, n \gt 2$ and $p_1, \cdots, p_n \in $ and $x_1, \cdots, x_n$.
            We show $f\paren{\sum_{i=1}^{n}{p_i x_i}} \le \sum_{i=1}^{n}{p_i f(x_i)}$ by induction. \\
            The definition of convexity yields the base case $i = 2$: $f(p_1 x_1 + p_2 x_2) \le p_1 f(x_1) + p_2 f(x_2)$. \\
            We assume the claim holds for $n - 1$ and the induction case goes as follows: \\
            \begin{align*}
                f\paren{\sum_{i=1}^{n}{p_i x_i}} &= f\paren{p_1 x_1 + (1 - p_1) \sum_{i=2}^{n}{\frac{p_i}{1 - p_1} x_i}}                                                                        \\
                &\le p_1 f\paren{x_1} + (1 - p_1) f\paren{\sum_{i=2}^{n}{\frac{p_i}{1 - p_1} x_i}}           && \text{(def. of convexity)}                                                      \\
                &\le p_1 f\paren{x_1} + (1 - p_1) \sum_{i=2}^{n}{\frac{p_i}{1 - p_1} f\paren{{x_i}}}         
                && \text{(induct. hypo. applies bc. of $\sum_{i = 2}^{n}{\frac{p_i}{1 - p_1}}$ = 1)} \\
                &= \sum_{i=1}^{n}{p_i f\paren{{x_i}}}
            \end{align*}
            If $\mathcal{X}$ is finite, the claim follows.
            If $\mathcal{X}$ is countably infinite, the claim follows using ... .
            % TODO: Case missing?
        \item
            Follows from part 1 applied to $-f$.
    \end{enumerate}
\end{proof}

\begin{corollary}
    Entropy and Mutual Information are non-negative:
    \begin{enumerate}
        \item $0 \le H(X)$
        \item $0 \le D(p(x) \| q(x))$
        \item $0 \le I(X; Y)$
    \end{enumerate}
\end{corollary}
\begin{proof}
    \begin{enumerate}
        \item 
            Note that $\log(\frac{1}{[0, 1]}) = \log([1, \infty]) = [0, \infty]$ 
            and $p(X)(\mathcal{X}) \in [0, 1]$. \\
            Using the monotonicity of the expected value, we obtain
            \begin{equation*}
                0 \le \mathbb{E}\paren{\log \paren{\frac{1}{p(X)}}}
                = -\mathbb{E}\paren{\log \paren{p(X)}} = H(X)
            \end{equation*}

        \item 
            We can prove this using Jensens inequality on a \textit{concave} function: % TODO: Make sure we use the support correctly.
            \begin{align*}
                -D(p(x) \| q(x)) &= -\mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}}         && \text{(def. of relative entropy)}            \\
                &\le -\log\paren{\mathbb{E}_p \frac{p(X)}{q(X)}}                        
                = \log\paren{\mathbb{E}_p \frac{q(X)}{p(X)}}                            && \text{(-log is convex, property of log)}     \\
                &= \log\paren{\sum_{x \in \mathcal{X}}{p(x) \frac{q(x)}{p(x)}}}         
                = \log\paren{\sum_{x \in \mathcal{X}}{q(x)}}                            && \text{(def. of exp. value, simplify expr.)}  \\
                &= \log\paren{1} = 0                                                    && \text{(q is a prob. function)}               \\
            \end{align*}
            So equivalently, we have $D(p(x) \| q(x)) \ge 0$.
        \item Follows from part 1: $I(X; Y) = D(p(x, y) \| p(x)p(y)) \ge 0$.
    \end{enumerate}
\end{proof}

\begin{corollary}
    \begin{enumerate}
        \item More information can only \textit{decrease} entropy: $H(X \mid Y) \le H(X)$.
        \item The uniform distribution maximizes entropy: \\
            $H(X) \le \log |\mathcal{X}|$ and $H(X) = \log |\mathcal{X}| \iff X \sim U(\mathcal{X})$.
        \item $p \mapsto H(p)$ is concave.
    \end{enumerate}
\end{corollary}
\begin{proof}
    \begin{enumerate}
        \item $0 \le I(X; Y) = H(X) - H(X \mid Y) \iff H(X \mid Y) \le H(X)$
        \item
            Let $Y \sim U(\mathcal{X})$ st. $\forall x \in \mathcal{X}\vcentcolon q(x)=\frac{1}{|\mathcal{X}|}$.
            \begin{align*}
                0 &\le D(p(x) \| q(x)) \\
                &= \mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}} \\
                &= \sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}} \\
                &= \sum_{x \in \mathcal{X}}{p(x) \log\paren{p(x)|\mathcal{X}|}} \\
                &= \sum_{x \in \mathcal{X}}{p(x) \log p(x)} + |\mathcal{X}| \sum_{x \in \mathcal{X}}{p(x)}\\
                &= -H(X) + |\mathcal{X}| = |\mathcal{X}| - H(X)
            \end{align*}
            This is equivalent to $H(X) \le \log |\mathcal{X}|$.
        \item
            \begin{align*}
                H(p) = -p \log p = % TODO: Idk how interesting is this...
            \end{align*}
    \end{enumerate}
\end{proof}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.50\linewidth]{../plots/entropy_function.png}
%     \caption{\label{fig:entropy_function}The entropy function is a concave function.}
% \end{figure}

% TODO: Add deep learning example.

\newpage
\section{Axiomatic Definition}


\bibliographystyle{alpha}
\bibliography{sample}

\end{document}
