\documentclass{article}

\usepackage[english]{babel}
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath, amsfonts, graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{dsfont, mathtools, amsthm, diagbox, caption, subcaption}
\usepackage[
    backend=bibtex,
    style=alphabetic,
]{biblatex}
\addbibresource{../bibliography.bib} %Imports bibliography file

\title{Introduction to Entropy}
\author{Erik Stern}

\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\paren}[1]{\left(#1\right)}
\DeclareMathOperator{\supp}{supp}
\newcommand{\injoint}{\in \mathcal{X} \times \mathcal{Y}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\begin{document}
\maketitle

% TODO: Clean this up.
\begin{abstract}
    This seminar paper provides a foundational overview of Information Theory, focusing on the mathematical properties of Shannon Entropy and Mutual Information. 
    We establish key definitions and rigorously prove fundamental inequalities, including the non-negativity of entropy and the Data Processing Inequality, 
    using Jensen's Inequality. To build intuition, theoretical results are complemented by numerical simulations of Bernoulli and Geometric random variables. 
    Finally, we introduce Relative Entropy (KL-Divergence) and discuss its geometric properties and applications in optimization.
\end{abstract}

\tableofcontents

\section{Entropy and Mutual information}
\subsection{Definitions and Conventions}

\begin{definition}
    Let $(\Omega, \mathcal{A}, \mathds{P})$ be a probability space, $\mathcal{X}$ be countable sets and
    $X: \Omega \rightarrow \mathcal{X}$ be a discrete random variable on $(\Omega, \mathcal{A}, \mathds{P})$.
    We can then define
    \begin{align*}
        \textbf{Entropy wrt. base:}       &\quad H_q(X) = \mathbb{E}\paren{-\log_q p(X)} = -\sum_{x \in \supp(X)}{p(x) \log_q p(x)} \\
        \textbf{Entropy conventionally:}  &\quad H(X) = H_2(X)
    \end{align*}
\end{definition}

\begin{remark}
    \begin{enumerate}
    \item
        Let $\mathcal{X}, \mathcal{Y}$ be countable sets and $X: \Omega \rightarrow \mathcal{X}$, $Y: \Omega \rightarrow \mathcal{Y}$ 
        be discrete random variables on $(\Omega, \mathcal{A}, \mathds{P})$.
        From now on, the random variables $X, Y$ are always available for use.

    \item
        We do \textbf{not} use the shorthand notations $p(x) = \mathds{P}[X = x]$ and $p(y) = \mathds{P}[Y = y]$
        from \cite{Thomas:ElementsOfInformationTheory}, to keep the notation easily understandable.

    \item
        Currently, $\mathcal{X}, \mathcal{Y} \subset \mathbb{R}$ is not required.  
        Later theorems like Jensens Inequality do require real-valued random variables.

    \item We use the convention $\log = \log_2$, as the entropy $H$ is defined wrt. base $2$.

    \item We also use the following convention and justify it through a continuity argument:
        \begin{align*}
            0 \log 0 = \lim_{x \to 0^+}{x \log x}
            = \lim_{x \to 0^+}\frac{\log x}{\frac{1}{x}}
            = \lim_{x \to 0^+}\frac{\frac{1}{\ln(2) x}}{-\frac{1}{x^2}}
            = \lim_{x \to 0^+}\frac{-x}{\ln(2)} = 0
        \end{align*}
        This choice is sensible, as $\log x$ is not defined for negative $x$.
    
        \item The conventions, definitions and theorems are from Definitions, Theorems, Remarks and Exercises 
        in \textit{Elements of Information Theory, second edition} (see \cite{Thomas:ElementsOfInformationTheory}).
    \end{enumerate}
\end{remark}

\begin{remark}
    Note that if $|\mathcal{X}|$ is finite, $(\forall p \in \mathbb{R}_+: H_p(X) \; \text{finite})$
    and $H(X) \le |\mathcal{X}|$ (see Theorem \ref{theorem:more-entropy-observations}).
    For $|\mathcal{X}|$ countably infinite, there are counterexamples where $H_p(X) = \infty$ (see \cite{MathStackexchange:CanEntropyBeInfinite}).
    From now on, we will assume that entropy is finite.
\end{remark}

\newpage
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{../plots/entropy_weighted_coin.pdf}
        \caption{Bernoulli Rand. Variable Entropy $H(X_p)$}
        \label{fig:bernoulli-entropy}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{../plots/entropy_geo_variable.pdf}
        \caption{Geometric Rand. Variable Entropy $H(X_p)$}
        \label{fig:geometric-entropy}
    \end{subfigure}
\end{figure}

\begin{example}
    Let $p \in (0, 1)$ and $X_p \sim B(1, p)$ be a weighted coin flip. \\
    We can calculate the Entropy of $X_p$: $H(X_p) = -p \log p - (1-p) \log (1-p)$. \\
    A visual inspection (see Figure \ref{fig:bernoulli-entropy}) reveals that $H(X_p)$ seems to be maximised for $p$ and minimised for $p \in \{0, 1\}$.
    An increase in uncertainty about the result of the coin flip seems to correspond with an increase in entropy.
\end{example}

\begin{example}
    Let $p \in (0, 1)$ and $X_p \sim G(p)$ be the number of times a weighted coin is flipped, until the first head occurs.
    We will calculate the Entropy of $X_p$. It will require the two well-known series:
    \begin{align}
        \forall r \in (0, 1)&\vcentcolon \sum_{n \in \mathbb{N}_0}{r^n} = \frac{1}{1 - r} \label{series:nexpr-n}\\
        \forall r \in (0, 1)&\vcentcolon \sum_{n \in \mathbb{N}_0}{n r^n} = \frac{r}{(1 - r)^2} \label{series:expr-n}
    \end{align}
    We can now directly calculate the Entropy of $X_p$:
    \begin{align*}
        H(X_p) &= \sum_{x \in \mathbb{N}}{-p(x) \log p(x)}                                  && \text{(Def. of Entropy)}\\
        &= -\sum_{x \in \mathbb{N}}{(1-p)^{x-1} p \log\paren{(1-p)^{x-1} p}}                && \text{(Subst. in geometric density function)} \\
        &= -\sum_{x \in \mathbb{N}}{(1-p)^{x-1} p \paren{(x-1) \log(1-p) + \log p}}         && \text{(Log rules)} \\
        &= -p \log(1-p) \sum_{x \in \mathbb{N}_{0}}\paren{(1-p)^{x} x} - p \log p \sum_{x \in \mathbb{N}_{0}}\paren{(1-p)^{x}}      && \text{(Factor out constants)} \\
        &= -p \log(1-p) \frac{1-p}{(1 - (1-p))^2} - p \log p \frac{1}{1 - (1-p)}            && \text{(Use series \ref{series:nexpr-n} and \ref{series:expr-n})}\\
        &= -p \log(1-p) \frac{1-p}{p^2} - p \log p \frac{1}{p}                              && \text{(Simplify expr.)} \\
        &= \frac{-(1-p) \log(1-p) - p \log p}{p}                                            && \text{(Simplify expr.)}
    \end{align*}
    For $p=0.5$ we get $H(X_{0.5}) = \frac{-0.5 \log 0.5 - 0.5 \log 0.5}{0.5} = -2 \log 0.5 = 2$. \\
    We can visually inspect $(0, 1) \rightarrow \mathbb{R}, p \mapsto H(X_p)$ (see Figure \ref{fig:geometric-entropy}) to get a feeling for the entropy of $X_p$.
    An increase in $p$ is linked to lower variance and more concentration of the distribution towards zero. 
    Based on the plot, that increase looks to be linked to a lower entropy and vice-versa.

    TODO: Complete the q
\end{example}

\newpage
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{../plots/pointwise_rel_entropy_continuation.pdf}
        \caption{Pointwise Relative Entropy}
        \label{fig:pointwise-rel-entropy}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{../plots/relative_entropy_binomial_distributions.pdf}
        \caption{Relative Entropies of Binomial Distributions}
        \label{fig:rel-entropy-bionm-distrib}
    \end{subfigure}
\end{figure}


\begin{definition}\label{def:entropy-mutual-info-def}
    \begin{align*}
    \intertext{\noindent We define the following:}
        \textbf{Conditional Entropy:} & \quad H(X \mid Y) = -\mathbb{E}\paren{\log p_{(X, Y)}(X \mid Y)} \\
        \textbf{Joint Entropy:}       & \quad H(X, Y) = -\mathbb{E}\paren{\log p_{(X, Y)}(X, Y)}
    \intertext{\noindent Let $p$ and $q$ be two probability mass functions on the same set $\mathcal{Z}$. We define the following:}
        \textbf{Relative Entropy:}    & \quad D(p \| q) = \mathbb{E}_{X \sim p}\paren{\log \frac{p(X)}{q(X)}} 
            \, \text{with conventions from Remark \ref{remark:relative-entropy-conventions}}
    \intertext{(Sometimes also called KL-Divergence)}
    \intertext{\noindent Using Relative Entropy, we can define the Mutual Information between the variables $X, Y$:}
        \textbf{Mutual Information:}  & \quad I(X; Y) = D(p_{(X, Y)} \| p_X \, p_Y) \\
    \end{align*}
\end{definition}

\begin{remark} \label{remark:relative-entropy-conventions}
    We have
    \begin{align*}
        D(p \| q)   &= \mathbb{E}_{X ~ p}\paren{\log \frac{p(X)}{q(X)}}             && \text{(def.~of relative entropy)} \\
                    &= \sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}}        && \text{(def.~of expected value)}
    \end{align*}
    To understand the conventions, we can look at the limit cases:
    \begin{enumerate}
        \item Case $p \in (0, 1], q = 0$: $\lim_{q \to 0^+} p \log \frac{p}{q} = \lim_{q \to 0^+} (p \log p - p \log q) = \infty$.
        \item Case $p = 0, q \in (0, 1]$: $0 \log \frac{0}{q} = 0$.
        \item Case $p = q = 0$: Case 1 logic yields $\lim_{q \to 0^+} p \log \frac{p}{q} = \infty$ and Case 2 logic yields $0 \log \frac{0}{0} = 0$. 
    \end{enumerate}
    As we want $\sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}}$ to sum over $x \in \mathcal{X}, p(x) > 0$, we choose the convention $0 \log \frac{0}{0} = 0$.
    Figure \ref{fig:pointwise-rel-entropy} visualizes the the pointwise relative entropy function $(p, q) \mapsto \log \frac{p}{q}$.
\end{remark}

\begin{example}
    To understand the concept, we can now calculate the relative entropies for an example.
    Let $X \sim B(20, \alpha)$ and $Y \sim B(20, \beta)$ with $\paren{\alpha, \beta} \in [0, 1]^2$.
    \begin{align*}
                                 & \quad D(p_X \| p_Y) = \sum_{x = 0}^{20}{p_X(x) \log \frac{p_X(x)}{p_Y(x)}} \\
        \alpha=0, \beta=1:       & \quad D(p_X \| p_Y) = 0 \log \frac{0}{1} + 1 \log \frac{1}{0} = 0 + \infty = \infty \\
        \alpha=0.1, \beta=0.9:   & \quad D(p_X \| p_Y) \approx 50.7 \\
        \alpha=0.3, \beta=0.7:   & \quad D(p_X \| p_Y) \approx 9.8 \\
        \alpha= 0.5, \beta= 0.5: & \quad D(p_X \| p_Y) = \sum_{x = 0}^{20}{p(x) \log 1} = 0 \\
    \end{align*}
    Figure \ref{fig:rel-entropy-bionm-distrib} visualizes the two discrete distribution functions in the cases above.
    Intuitively, the more overlap the distributions have, the closer to zero the relative entropy is.
\end{example}


\newpage
\subsection{Mutual Information and Chain Rules}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{../plots/entropy_mutual_info_venn_diagram.pdf}
    \caption{Relationship between Entropy, Conditional Entropy and Mutual Information}
    \label{fig:relationships-entropy-mutual-information}
\end{figure}

\begin{theorem}
    There are multiple equivalent ways to express Mutual Information (see Figure \ref{fig:relationships-entropy-mutual-information}):
    \begin{enumerate}
        \item $I(X; Y) = H(Y) - H(Y | X)$
        \item $I(X; Y) = I(Y; X)$
        \item $I(Y; X) = H(X) - H(X | Y)$
        \item $I(X; Y) = H(X, Y)$
        \item $I(X; X) = H(X)$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item We can use the definition of mutual information and relative entropy to obtain:
            \begin{align*}
                I(X; Y) &= D(p(x, y) \| p(x)p(y))                                                           && \text{(by def.~of mutual info.)}\\
                        &= \mathbb{E}_{p(x, y)}\paren{\log \frac{p(x, y)}{p(x)p(y)}}                        && \text{(by def.~relative entropy)} \\
                        &= \mathbb{E}_{p(x, y)}\paren{\log \frac{p(x)p(y|x)}{p(x)p(y)}}                     && \text{(using cond.~probability)} \\
                        &= \mathbb{E}_{p(x, y)}\paren{\log \frac{p(y|x)}{p(y)}}                             && \text{(simplify fraction)} \\
                        &= \mathbb{E}_{p(x, y)}\paren{\log p(y|x)} - \mathbb{E}_p(x, y)\paren{\log p(y)}    && \text{(simplify logarithm)} \\
                        &= H(Y | X) - H(Y)                                                                  && \text{(by def.~of entropy)}
            \end{align*}
        \item The definition of mutual information yields:
            \begin{align*}
                I(X; Y) &= D(p(x, y) \| p(x)p(y))                       && \text{(by def.~of mutual info.)} \\
                        &= D(P[X = x, Y = y] \| P[X = x]P[Y = y])       && \text{(use clear notation)} \\
                        &= D(P[Y = y, X = x] \| P[Y = y]P[X = x])       && \text{(use commutativity)} \\
                        &= D(p(y, x) \| p(y)p(x))                       && \text{(back to conventional notation)}\\
                        &= I(X; Y)
            \end{align*}
        \item Follows directly from 2 and 3.
        \item \begin{align*}
                I(X; Y) &= H(Y) - H(X | Y)                  && \text{(by 1)} \\
                        &= H(Y) - \paren{H(X, Y) - H(X)}    && \text{(chain rule)} \\
                        &= H(X) + H(Y) - H(X, Y)
            \end{align*}
        \item Using 1 we get $I(X; X) = H(X) - H(X | X) = H(X)$.
    \end{enumerate}
\end{proof}

\newpage
\begin{definition}\label{def:entropy-mutual-info-def}
    Let $p$ and $q$ be probability mass functions on $\mathcal{X} \times \mathcal{Y}$.
    \begin{align*}
    \intertext{\noindent We define the following:}
        \textbf{Conditional Relative Entropy:}    & \quad D(p_{Y \mid X} \| q_{Y \mid X}) \\
        &= \mathbb{E}_{X \sim p_X} D(p(Y \mid X) \| q(Y \mid X)) \\
        &= \sum_{x \in \mathcal{X}}{p(x) D(p(y \mid x) \| q(y \mid x))} \\
        &= \mathbb{E}_{(X, Y) \sim p(x, y)}\paren{\log \frac{p(Y \mid X)}{q(Y \mid X)}} \\
        &= \sum_{(x, y) \injoint}{p(x, y) \log \frac{p(y \mid x)}{q(y \mid x)}} \\
        \textbf{Conditional Mutual Information:}  & \quad I(X; Y \mid Z) 
        = H(X \mid Z) - H(X \mid Y, Z)
    \end{align*}
\end{definition}

\begin{example}
    The Conditional Relative Entropy can be illustrated using a Christmas example: \\
    Let $\Omega = \{1, \cdots, 12\}$, $\mathds{P}$ uniformly distributed, \\
    $\mathcal{X} = \{\textbf{JANUARY}, \cdots, \textbf{DECEMBER}\}$ and
    $\mathcal{Y} = \{0, 1\}$. \\
    Our variable $X$ gives us the month of the year: \\
    We define it as $X: \Omega \rightarrow \mathcal{X}, \omega \mapsto \textbf{the corresponding month}$. \\
    Our variable $Y$ answers the question "Is Christmas Advent?": \\
    We define it as $Y: \Omega \rightarrow \mathcal{Y}, \omega \mapsto \mathds{1}_{\{\textbf{NOVEMBER}, \textbf{DECEMBER}\}}(\omega)$. \\
    We have lived on earth our entire lives. Our model of reality $p$ is correct! \\
    We define it as
    $$
        p(Y = 1 \mid x) = \begin{cases}
			1, & \text{if $x \in \{\textbf{NOVEMBER}, \textbf{DECEMBER}\}$} \\
            0, & \text{otherwise}
		\end{cases}
    $$
    The alien \textit{ET} visits earth. He sees the Christmas trees in January and assumes Advent continues into January.
    In November and December, he is uncertain too. His model of reality $q$ is not correct! \\
    We define it as 
    $$
        q(Y = 1 \mid x) = \begin{cases}
			0.5, & \text{if $x \in \{\textbf{NOVEMBER}, \textbf{DECEMBER}, \textbf{JANUARY}\}$} \\
            0, & \text{otherwise}
		\end{cases}
    $$
    Let us calculate the individual relative entropies:
    \begin{enumerate}
        \item $D(p(y \mid \textbf{NOV.}) \| q(y \mid \textbf{NOV.})) = 0 + 1 \log \frac{1}{0.5} = 1$
        \item $D(p(y \mid \textbf{DEC.}) \| q(y \mid \textbf{DEC.})) = 0 + 1 \log \frac{1}{0.5} = 1$
        \item $D(p(y \mid \textbf{JAN.}) \| q(y \mid \textbf{JAN.})) = 1 \log \frac{1}{0.5} + 0 = 1$
        \item Otherwise: $D(p(y \mid x) \| q(y \mid x)) = 0$
    \end{enumerate}
    We can now calculate the Conditional Relative Entropy:
    \begin{align*}
        D(p_{Y \mid X} \| q_{Y \mid X}) &= \sum_{x \in \mathcal{X}}{p_X(x) D(p(y \mid x) \| q(y \mid x))} \\
        &= \frac{1}{12} \paren{1 + 1 + 1} = \frac{3}{12} = 0.25
    \end{align*}
    So the expression calculates how much of an error \textit{ET} makes per month, on average.
\end{example}

% TODO: Idk if this example is useful. Let's see.
% \begin{example}
%     To understand the concept, we can now calculate the Conditional Mutual Information for an example.
%     % TODO: Fill in this vertical.
% \end{example}


\newpage
% TODO: Explain all of these notations first.

\begin{theorem}\label{theorem:chain-rules}
    Let $n \in \mathbb{N}, n \ge 2$, $(X_1, \cdots, X_n) \sim p(x_1, \cdots, x_n)$ and $Y$ a random variable.
    The following statements about Entropy and Mutual Information are called Chain Rules:
    \begin{enumerate}
        \item $H(X_1, \cdots, X_n) = \sum_{i=1}^{n}{H(X_i \mid X_{i-1}, \cdots, X_1)}$
        \item $D(p(x) \| q(x)) = D(p(x \mid y) \| q(x \mid y)) + D(p(y) \| q(y))$
        \item $I(X_1, \cdots, X_n; Y) = \sum_{i=1}^{n}{I(X_i; Y \mid X_{i-1}, \cdots, X_1)}$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item 
            Prove this result using induction by $n$. \\
            Base case $n = 2$:
            \begin{align*}
                H(X_1, X_2) &= -\mathbb{E}\paren{\log p(X_1, X_2)} \\
                &= -\mathbb{E}\paren{\log p(X_2 \mid X_1)p(X_1)} \\
                &= -\mathbb{E}\paren{\log p(X_2 \mid X_1)} + -\mathbb{E}\paren{\log p(X_1)} \\
                &= H(X_1) + H(X_2 \mid X_1)
            \end{align*}
            Assume the theorem holds for $n - 1$. Induction case $n - 1$ to $n$:
            \begin{align*}
                H(X_1, \cdots, H_n) &= H(X_n | X_1, \cdots, H_{n-1}) + H(X_1, \cdots, H_{n-1})          && \text{(apply base case)} \\
                &= H(X_n | X_1, \cdots, H_{n-1}) + \sum_{i=1}^{n-1}{H(X_i \mid X_{i-1}, \cdots, X_1)}   && \text{(induction hypothesis)}
            \end{align*}

        \item
            \begin{align*}
                D(p(x) \| q(x)) &= \sum_{(x, y) \injoint}{p(x, y) \frac{p(x)}{q(x)}}
                = \sum_{(x, y) \injoint}{p(x, y) \frac{p(x)}{q(x)}}
                = \sum_{(x, y) \injoint}{p(x, y) \frac{p(x \mid y)p(y)}{q(x \mid y)p(y)}} \\
                &= \sum_{(x, y) \injoint}{p(x, y) \frac{p(x \mid y)}{q(x \mid y)}} + \sum_{(x, y) \injoint}{p(x, y) \frac{p(y)}{p(y)}} \\
                &= D(p(x \mid y) \| q(x \mid y)) + D(p(y) \| q(y))
            \end{align*}
        
        \item
            \begin{align*}
                I(X_1, \cdots, X_n; Y) &= H(X_1, \cdots, X_n) - H(X_1, \cdots, X_n \mid Y) \\
                &= \sum_{i=1}^{n}{H(X_i \mid X_{i-1}, \cdots, X_1)} + \sum_{i=1}^{n}{H(X_i \mid X_{i-1}, \cdots, X_1, Y)} \\
                &= \sum_{i=1}^{n}{I(X_i; Y \mid X_{i-1}, \cdots, X_1)}
            \end{align*}

    \end{enumerate}
\end{proof}

% TODO: Definition of relative entropy correctly uses E_p everywhere instead of E.

\begin{corollary}
    The following specialization of the Chain Rule for Entropy is commonly used: $H(X, Y) = H(X) + H(Y \mid X)$
\end{corollary}
\begin{proof}
    Follows from Theorem~\ref{theorem:chain-rules}.
\end{proof}



\newpage
\section{Inequalities for Entropy and Mutual Information}
\subsection{Convexity and Jensens Inequality}

\begin{remark}
    We use the common definition of convex functions, concave functions from analysis.
    From now on, $X, Y$ have to be real-valued random variables.
\end{remark}

% TODO: Make sure which theorems and definitions assume discrete.

\begin{theorem}
    Let $f: \Omega \rightarrow \mathbb{R}$ a function.
    \begin{enumerate}
        \item If $f$ is convex, we have $\mathbb{E} f(X) \ge f(\mathbb{E} X)$.
        \item If $f$ is concave, we have $\mathbb{E} f(X) \le f(\mathbb{E} X)$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item
            Let $n \in \mathbb{N} \setminus \{1\}$ and $p_1, \cdots, p_n \in (0, 1)$ such that $\sum_{i=1}^{n}{p_i} = 1$ and $x_1, \cdots, x_n \in \mathbb{R}$.
            Distinguish two cases. If $\mathcal{X}$ is finite:
            We show $f\paren{\sum_{i=1}^{n}{p_i x_i}} \le \sum_{i=1}^{n}{p_i f(x_i)}$ by induction. \\
            The definition of convexity yields the base case $i = 2$: $f(p_1 x_1 + p_2 x_2) \le p_1 f(x_1) + p_2 f(x_2)$. \\
            We assume the claim holds for $n - 1$ and the induction case goes as follows: \\
            \begin{align*}
                f\paren{\sum_{i=1}^{n}{p_i x_i}} &= f\paren{p_1 x_1 + (1 - p_1) \sum_{i=2}^{n}{\frac{p_i}{1 - p_1} x_i}}                                                                        \\
                &\le p_1 f\paren{x_1} + (1 - p_1) f\paren{\sum_{i=2}^{n}{\frac{p_i}{1 - p_1} x_i}}           && \text{(def. of convexity)}                                                      \\
                &\le p_1 f\paren{x_1} + (1 - p_1) \sum_{i=2}^{n}{\frac{p_i}{1 - p_1} f\paren{{x_i}}}         
                && \text{(induct. hypo. applies bc. of $\sum_{i = 2}^{n}{\frac{p_i}{1 - p_1}}$ = 1)} \\
                &= \sum_{i=1}^{n}{p_i f\paren{{x_i}}}
            \end{align*}
            Else: \\% If $\mathcal{X}$ is countably infinite, the claim follows using ... .
            % TODO: Case missing?
        \item
            Follows from part 1 applied to $-f$.
    \end{enumerate}
\end{proof}

\begin{corollary}
    Entropy and Mutual Information are non-negative:
    \begin{enumerate}
        \item $0 \le H(X)$
        \item $0 \le D(p(x) \| q(x))$
        \item $0 \le I(X; Y)$
    \end{enumerate}
\end{corollary}
\begin{proof}
    \begin{enumerate}
        \item 
            Note that $\log(\frac{1}{[0, 1]}) = \log([1, \infty]) = [0, \infty]$ 
            and $p(X)(\mathcal{X}) \in [0, 1]$. \\
            Using the monotonicity of the expected value, we obtain
            \begin{equation*}
                0 \le \mathbb{E}\paren{\log \paren{\frac{1}{p(X)}}}
                = -\mathbb{E}\paren{\log \paren{p(X)}} = H(X)
            \end{equation*}

        \item 
            We can prove this using Jensens Inequality on a \textit{concave} function: % TODO: Make sure we use the support correctly.
            \begin{align*}
                -D(p(x) \| q(x)) &= -\mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}}         && \text{(def. of relative entropy)}            \\
                &\le -\log\paren{\mathbb{E}_p \frac{p(X)}{q(X)}}                        
                = \log\paren{\mathbb{E}_p \frac{q(X)}{p(X)}}                            && \text{(-log is convex, property of log)}     \\
                &= \log\paren{\sum_{x \in \mathcal{X}}{p(x) \frac{q(x)}{p(x)}}}         
                = \log\paren{\sum_{x \in \mathcal{X}}{q(x)}}                            && \text{(def. of exp. value, simplify expr.)}  \\
                &= \log\paren{1} = 0                                                    && \text{(q is a prob. function)}
            \end{align*}
            So equivalently, we have $D(p(x) \| q(x)) \ge 0$.
        \item Follows from part 1: $I(X; Y) = D(p(x, y) \| p(x)p(y)) \ge 0$.
    \end{enumerate}
\end{proof}

\begin{remark}
    There are multiple natural questions we can ask about Entropy.
    We will look at an example for each of them and then prove the results in the follow-up theorem.
    \begin{enumerate}
        \item What distribution maximises the value of Entropy?
            I simulated 

            From the data, it seems plausible that the uniform distribution maximises entropy. We
            % TODO: How difficult is it to sample from the simplex?
        \item Can joint entropy increase if we add redundant information? \\
            Let $\Omega = \{1, 2\}$, p uniform TODO, $X, Y: \Omega \rightarrow \mathbb{R}$, $X(\omega) = \omega$ and $Y(\omega) = 2\omega$.
            $Y$ is redundant to $X$, as $Y = 2X$. \\
            % We can now calculate the entropy to verify the intuition, that adding $Y
            \begin{align*}
                H(X) &= \sum_{i = 1}^{2}{-0.5 \log 0.5} = -\log 0.5 = \log 2 = 1 \\
                H(X, Y) &= \sum_{(i, j) \in \{(1, 2), (2, 4)\}}{-0.25 \log 0.25} = 2 * 0.25 * 2 = 1
                % TODO: These sums could maybe be expressed more formally first and then substituted in
            \end{align*}
            Pairs like $(1, 4)$ have probability zero and do not contribute to the sum in $H(X, Y)$.
            This example suggest entropy never increases. We will prove that in the follow-up theorem.
        \item What happens to the Entropy if we add independent noise to our measurements? \\
            Let $X \sim U(\{1, 2, 3, 4\})$ be the original signal, $N \sim B(3, 0.5)$ the noise and let $X, N$ be independent. 
            Then $S \defeq X + N$ is a noisy signal. \\

            We can compute the critical variance $a$.
            % TODO
    \end{enumerate}
\end{remark}

\begin{theorem} \label{theorem:more-entropy-observations}
    We can formalise the previous observations including some more:
    \begin{enumerate}
        \item More information can only \textit{decrease} entropy: $H(X \mid Y) \le H(X)$.
        \item The uniform distribution maximizes entropy: \\
            $H(X) \le \log |\mathcal{X}|$ and $H(X) = \log |\mathcal{X}| \iff X \sim U(\mathcal{X})$.
        \item If information from $Y$ does not add anything to $X$, then $Y$ must be derived from $X$: \\ % TODO: Dis true backwards?
            $H(Y \mid X) = 0 \implies \exists f\vcentcolon Y = f(X) \; \text{almost surely}$
        \item If independent noise is added to a random variable, entropy can only increase: \\
            Set $Z = X + Y$. Then we have $X, Y \text{independent} \implies H(X) \le H(Z) \land H(Y) \le H(Z)$
        \item TODO: Maybe add independence bound?
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item $0 \le I(X; Y) = H(X) - H(X \mid Y) \iff H(X \mid Y) \le H(X)$

        \item
            Let $Y \sim U(\mathcal{X})$ st. $\forall x \in \mathcal{X}\vcentcolon q(x)=\frac{1}{|\mathcal{X}|}$.
            \begin{align*}
                0 &\le D(p(x) \| q(x))
                = \mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}}
                = \sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}} \\
                &= \sum_{x \in \mathcal{X}}{p(x) \log\paren{p(x)|\mathcal{X}|}}
                = \sum_{x \in \mathcal{X}}{p(x) \log p(x)} + \log |\mathcal{X}| \sum_{x \in \mathcal{X}}{p(x)}\\
                &= -H(X) + \log |\mathcal{X}| = \log |\mathcal{X}| - H(X)
            \end{align*}
            This is equivalent to $H(X) \le \log |\mathcal{X}|$.
            Lastly, show that $H(X) = \log |\mathcal{X}| \iff X \sim U(\mathcal{X})$. \\
            $"\Leftarrow":$ This follows from the calculation above. \\
            $"\Rightarrow":$
            \begin{align*}
                \log |\mathcal{X}| = -\sum_{x \in \mathcal{X}}{p(x) \log q(x)}
                \iff \log\frac{1}{|\mathcal{X}|} = \sum_{x \in \mathcal{X}}{p(x) \log q(x)} \\
                \iff \sum_{x \in \mathcal{X}}{p(x) \log\frac{1}{|\mathcal{X}|}} = \sum_{x \in \mathcal{X}}{p(x) \log q(x)} \\
                % TODO
            \end{align*}

        \item
            We have $H(Y \mid X) = -\mathbb{E}\paren{\log p(Y \mid X)} = \sum_{(x, y) \injoint}{-p(x, y) \log p(y \mid x)}$. \\
            Additionally, we have $\forall (x, y) \in \mathcal{X} \times \mathcal{Y}: -p(x, y) \log p(y \mid x) \ge 0$. \\
            Combining those facts, we get
            \begin{align*}
                &H(Y \mid X) = 0 \\
                \iff &\forall (x, y) \injoint: p(x, y) \log p(y \mid x) = 0 \\
                \iff &\forall (x, y) \injoint: p(x, y) = 0 \oplus \log p(y \mid x) = 0 \\
                \iff &\forall (x, y) \injoint: p(x, y) = 0 \oplus p(y \mid x) = 1
            \end{align*}
            This tells us, that either $p(x, y) = 0$ or $p(x, y) = p(y \mid x)p(x) = p(x) = 1$. \\
            Now we can finish up the argument. Set $A = \{x \in \mathcal{X}: p(x) > 0\}$. \\
            Define $(y_x)_{x \in \mathcal{X}}$ such that $\forall x \in \mathcal{X}: p(x, y_x) > 0$. \\
            Set $f: A \rightarrow \mathcal{Y}, x \mapsto y_x$. This gets us $Im f = \{y_x: x \in \mathcal{X}\} = \{y \in \mathcal{Y}: p(x, y) > 0\}$. \\
            So $Y = f(X) \; \text{almost surely}$.

        \item
            We have
            \begin{align*}
                 H(Z \mid X) &= -\mathbb{E}\paren{\log p(Z \mid X)} = \sum_{(x, y) \injoint}{-p(x, y) \log P[Z = x + y \mid X = x]} \\
                 &= \sum_{(x, y) \injoint}{-p(x, y) \log P[X + Y = x + y \mid X = x]} \\
                 &= \sum_{(x, y) \injoint}{-p(x, y) \log P[Y = y \mid X = x]} \\
                 &= -\mathbb{E}\paren{\log p(Y \mid X)} = H(Y \mid X)
            \end{align*}
            $H(Z, X) = H(X) + H(Z \mid X)$
            $H(Y, X) = H(X) + H(Y \mid X)$
            % TODO: Fix this shit!

            $H(Y, X) = H(X) + H(Y \mid X)$
            Using the Chain rule, we get $H(X, Y) = H(Y \mid X) + H(X)$

    \end{enumerate}
\end{proof}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.50\linewidth]{../plots/entropy_function.png}
%     \caption{\label{fig:entropy_function}The entropy function is a concave function.}
% \end{figure}

% TODO: Add deep learning example.

\newpage
\subsection{The Log-Sum Inequality}
%         \item $p \mapsto H(p)$ is concave.

Hello

\newpage
\subsection{Application to Optimisation}


% TODO: Relative entropy is often used as a distance, but not actually a metric.


Hello

\medskip
\printbibliography

\end{document}
