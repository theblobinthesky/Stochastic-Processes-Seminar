\documentclass[aspectratio=169]{beamer}

% Theme configuration - cleaner academic look
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{infolines}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{caption}[numbered]

% Custom colors based on University of Passau branding
\definecolor{unipassaublue}{RGB}{0,62,107}
\definecolor{unipassaugray}{RGB}{88,88,90}
\setbeamercolor{structure}{fg=unipassaublue}
\setbeamercolor{frametitle}{bg=unipassaublue,fg=white}
\setbeamercolor{title}{fg=unipassaublue}
\setbeamercolor{block title}{bg=unipassaublue,fg=white}
\setbeamercolor{block body}{bg=unipassaublue!10}
\setbeamercolor{item}{fg=unipassaublue}

% Logo in footer
\logo{\includegraphics[height=0.8cm]{uni-passau-logo.png}}

% Packages
\usepackage[english]{babel}
\usepackage{amsmath, amsfonts, amsthm, mathtools}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{caption}

% Macros from writeup/writeup.tex
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\paren}[1]{\left(#1\right)}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\argmin}{argmin}
\newcommand{\injoint}{\in \mathcal{X} \times \mathcal{Y}}

% Theorem environments
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{exmp}{Example}
\newtheorem{rmk}{Remark}

\title{Introduction to Entropy}
\subtitle{Stochastic Processes Seminar}
\author{Erik Stern}
\institute{University of Passau}
\date{\today}

% Section title slides - no text, just visual separator
\setbeamercolor{title}{fg=white}
\AtBeginSection[]{
    \begin{frame}
        \vfill
        \centering
        \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
            \usebeamerfont{title}\insertsectionhead\par
        \end{beamercolorbox}
        \vfill
    \end{frame}
}

\begin{document}

% Title frame
\begin{frame}
    \titlepage
\end{frame}

% Table of contents
\begin{frame}{Outline}
    \tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Entropy and Mutual Information}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Definition 1: Entropy}
    \begin{defn}
        Let $(\Omega, \mathcal{A}, \mathds{P})$ be a probability space, $\mathcal{X}$ a countable set, and
        $X: \Omega \rightarrow \mathcal{X}$ a discrete random variable.
        \begin{align*}
            \textbf{Entropy wrt. base:}       &\quad H_b(X) = \mathbb{E}\paren{-\log_b p_X(X)} = -\sum_{x \in \supp(X)}{p_X(x) \log_b p_X(x)} \\
            \textbf{Entropy conventionally:}  &\quad H(X) = H_2(X)
        \end{align*}
    \end{defn}
\end{frame}

\begin{frame}{Remark 1: Notation Conventions (1/2)}
    \begin{rmk}
        \begin{enumerate}
        \item
            Let $\mathcal{X}, \mathcal{Y}$ be countable sets and $X: \Omega \rightarrow \mathcal{X}$, $Y: \Omega \rightarrow \mathcal{Y}$ 
            be discrete random variables on $(\Omega, \mathcal{A}, \mathds{P})$.
            From now on, the random variables $X, Y$ are always available for use.

        \item
            We do \textbf{not} use the shorthand notations $p(x) = \mathds{P}[X = x]$ and $p(y) = \mathds{P}[Y = y]$,
            to keep the notation easily understandable.

        \item We use the convention $\log = \log_2$, as the entropy $H$ is defined wrt. base $2$.
        \end{enumerate}
    \end{rmk}
\end{frame}

\begin{frame}{Remark 1: Notation Conventions (2/2)}
    \begin{rmk}
        \begin{enumerate}
        \setcounter{enumi}{3}
            \item We also use the following convention and justify it through a continuity argument:
            \begin{align*}
                0 \log 0 = \lim_{x \to 0^+}{x \log x}
                = \lim_{x \to 0^+}\frac{\log x}{\frac{1}{x}}
                = \lim_{x \to 0^+}\frac{\frac{1}{\ln(2) x}}{-\frac{1}{x^2}}
                = \lim_{x \to 0^+}\frac{-x}{\ln(2)} = 0
            \end{align*}
            This choice is sensible, as $\log x$ is not defined for negative $x$.

            \item The conventions, definitions and theorems are from Definitions, Theorems, Remarks and Exercises 
            in \textit{Elements of Information Theory, second edition}.
        \end{enumerate}
    \end{rmk}
\end{frame}

\begin{frame}{Remark 2: Existence of Entropy}
    \begin{rmk}[Existence of Entropy]
        Note that if $|\mathcal{X}|$ is finite, $(\forall p \in \mathbb{R}_+: H_b(X) \; \text{finite})$
        and $H(X) \le |\mathcal{X}|$ (see Theorem 5).
        
        \vspace{0.5em}
        For $|\mathcal{X}|$ countably infinite, there are counterexamples where $H_b(X) = \infty$.
        
        \vspace{0.5em}
        From now on, we will assume that entropy is finite.
    \end{rmk}
\end{frame}

\begin{frame}{Example 1: Entropy of Bernoulli Variable}
    \begin{exmp}[Entropy of bernoulli variable]
        Let $p \in (0, 1)$ and $X_p \sim B(1, p)$ be a weighted coin flip. \\
        We can calculate the Entropy of $X_p$: $H(X_p) = -p \log p - (1-p) \log (1-p)$. \\
        A visual inspection reveals that $H(X_p)$ seems to be maximised for $p = 0.5$ and minimised for $p \in \{0, 1\}$.
        An increase in uncertainty about the result of the coin flip seems to correspond with an increase in entropy.
    \end{exmp}
    \begin{figure}
        \centering
        \includegraphics[width=0.5\linewidth]{../plots/entropy_weighted_coin.pdf}
        \caption{Bernoulli Rand. Variable Entropy $H(X_p)$}
    \end{figure}
\end{frame}

\begin{frame}{Example 2: Entropy of Geometric Variable (1/3)}
    \begin{exmp}[Entropy of geometric variable]
        Let $p \in (0, 1)$ and $X_p \sim G(p)$ be the number of times a weighted coin is flipped, until the first head occurs.
        We will calculate the Entropy of $X_p$. It will require the two well-known series:
        \begin{align}
            \forall r \in (0, 1)&\vcentcolon \sum_{n \in \mathbb{N}_0}{r^n} = \frac{1}{1 - r} \label{series:nexpr-n}\\
            \forall r \in (0, 1)&\vcentcolon \sum_{n \in \mathbb{N}_0}{n r^n} = \frac{r}{(1 - r)^2} \label{series:expr-n}
        \end{align}
    \end{exmp}
\end{frame}

\begin{frame}{Example 2: Entropy of Geometric Variable (2/3)}
    We can now directly calculate the Entropy of $X_p$:
    {\small
    \begin{align*}
        H(X_p) &= \sum_{x \in \mathbb{N}}{-p(x) \log p(x)}                                  && \text{(Def. of Entropy)}\\
        &= -\sum_{x \in \mathbb{N}}{(1-p)^{x-1} p \log\paren{(1-p)^{x-1} p}}                && \text{(Geometric mass func.)} \\
        &= -\sum_{x \in \mathbb{N}}{(1-p)^{x-1} p \paren{(x-1) \log(1-p) + \log p}}         && \text{(Log rules)} \\
        &= -p \log(1-p) \sum_{x \in \mathbb{N}_{0}}(1-p)^{x} x - p \log p \sum_{x \in \mathbb{N}_{0}}(1-p)^{x}      && \text{(Factor out)} \\
        &= -p \log(1-p) \frac{1-p}{p^2} - p \log p \frac{1}{p}                              && \text{(Use series \ref{series:nexpr-n}, \ref{series:expr-n})} \\
        &= \frac{-(1-p) \log(1-p) - p \log p}{p}                                            && \text{(Simplify)}
    \end{align*}
    }
\end{frame}

\begin{frame}{Example 2: Entropy of Geometric Variable (3/3)}
    For $p=0.5$ we get $H(X_{0.5}) = \frac{-0.5 \log 0.5 - 0.5 \log 0.5}{0.5} = -2 \log 0.5 = 2$.
    
    \begin{columns}
        \begin{column}{0.55\textwidth}
            {\small
            We can visually inspect $(0, 1) \rightarrow \mathbb{R}, p \mapsto H(X_p)$ to get a feeling for the entropy of $X_p$.
            An increase in $p$ is linked to lower variance and more concentration of the distribution towards zero. 
            Based on the plot, that increase looks to be linked to a lower entropy and vice-versa.

            \vspace{0.3em}
            A simple strategy to determine $X$ is to ask "Is $X=1$?", "Is $X=2$?" and so on.
            The number of questions required is exactly $X$.
            Thus, the average number of questions is $\mathbb{E}(X_{0.5}) = \frac{1}{0.5} = 2$. 
            This matches $H(X_{0.5}) = 2$.
            }
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\linewidth]{../plots/entropy_geo_variable.pdf}
                \caption{Geometric Entropy $H(X_p)$}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Definition 2: Conditional, Joint, Relative Entropy, Mutual Information}
    \begin{defn}
        \begin{align*}
            \textbf{Conditional Entropy:} & \quad H(X \mid Y) = -\mathbb{E}\paren{\log p_{(X \mid Y)}(X \mid Y)} \\
            &= \sum_{(x, y) \injoint}{p_{(X, Y)}(x, y) \log p_{(X \mid Y)}(x \mid y)} \\
            \textbf{Joint Entropy:}       & \quad H(X, Y) = -\mathbb{E}\paren{\log p_{(X, Y)}(X, Y)} && \text{(dito)}\\
            \textbf{Relative Entropy:}    & \quad D(p \| q) = \mathbb{E}_{X \sim p}\paren{\log \frac{p(X)}{q(X)}} \\
            &= \sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}}
                \quad \text{(also: KL-Divergence)} \\
            \textbf{Mutual Information:}  & \quad I(X; Y) = D(p_{(X, Y)} \| p_X \, p_Y)
        \end{align*}
        where $p, q$ are probability mass functions on the same set $\mathcal{X}$, with conventions from Remark 3.
    \end{defn}
\end{frame}

\begin{frame}{Remark 3: More Conventions}
    \begin{rmk}
        We have $D(p \| q) = \sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}}$.
        To understand the conventions, we look at limit cases:
        \begin{enumerate}
            \item Case $p \in (0, 1], q = 0$: $\lim_{q \to 0^+} p \log \frac{p}{q} = \lim_{q \to 0^+} (p \log p - p \log q) = \infty$.
            \item Case $p = 0, q \in (0, 1]$: $0 \log \frac{0}{q} = 0$.
            \item Case $p = q = 0$: Case 1 logic yields $\infty$ and Case 2 logic yields $0$. \\
                As we want $\sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}}$ to sum over $x \in \mathcal{X}, p(x) > 0$, we choose $0 \log \frac{0}{0} = 0$.
        \end{enumerate}
        If $p_Y(y) = 0$, we get $p_{(X, Y)}(x, y) = 0$. So $0 \log 0 = 0$ justifies that cond. entropy is well-defined.
    \end{rmk}
\end{frame}

\begin{frame}{Example 3: Relative Entropy of Binomial Distributions (1/2)}
    \begin{exmp}
        To understand the concept, we can now calculate the relative entropies for an example.
        Let $X \sim B(20, \alpha)$ and $Y \sim B(20, \beta)$ with $\paren{\alpha, \beta} \in [0, 1]^2$.
        \begin{align*}
                                         & \quad D(p_X \| p_Y) = \sum_{x = 0}^{20}{p_X(x) \log \frac{p_X(x)}{p_Y(x)}} \\
            \alpha=0, \beta=1:       & \quad D(p_X \| p_Y) = 1 \log \frac{1}{0} + 0 \log \frac{0}{1} = \infty + 0 = \infty \\
            \alpha=0.1, \beta=0.9:   & \quad D(p_X \| p_Y) \approx 50.7 \\
            \alpha=0.3, \beta=0.7:   & \quad D(p_X \| p_Y) \approx 9.8 \\
            \alpha= 0.5, \beta= 0.5: & \quad D(p_X \| p_Y) = \sum_{x = 0}^{20}{p(x) \log 1} = 0 \\
        \end{align*}
    \end{exmp}
\end{frame}

\begin{frame}{Example 3: Relative Entropy of Binomial Distributions (2/2)}
    \begin{figure}
        \centering
        \includegraphics[width=0.65\linewidth]{../plots/relative_entropy_binomial_distributions.pdf}
        \caption{Relative Entropies of Binomial Distributions}
    \end{figure}
    Intuitively, the more overlap the distributions have, the closer to zero the relative entropy is.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mutual Information and Chain Rules}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Theorem 1: Chain Rule for Entropy}
    \begin{thm}[Chain Rule for Entropy]
        We have $H(X, Y) = H(X) + H(Y \mid X)$
    \end{thm}
    \begin{proof}
        \begin{align*}
            H(X, Y) &= -\mathbb{E}\paren{\log p(X, Y)} 
            = -\mathbb{E}\paren{\log p(Y \mid X)p(X)} \\
            &= -\mathbb{E}\paren{\log p(Y \mid X)} - \mathbb{E}\paren{\log p(X)}
            = H(X) + H(Y \mid X)
        \end{align*}
    \end{proof}
\end{frame}

\begin{frame}{Theorem 2: Mutual Information Equivalences}
    \begin{figure}
        \centering
        \includegraphics[width=0.6\linewidth]{../plots/entropy_mutual_info_venn_diagram.pdf}
        \caption{Relationship between Entropy, Conditional Entropy and Mutual Information}
    \end{figure}
\end{frame}

\begin{frame}{Theorem 2: Mutual Information Equivalences (Statement)}
    \begin{thm}
        There are multiple equivalent ways to express Mutual Information:
        \begin{enumerate}
            \item $I(X; Y) = H(Y) - H(Y | X)$
            \item $I(X; Y) = I(Y; X)$
            \item $I(Y; X) = H(X) - H(X | Y)$
            \item $I(X; Y) = H(X) + H(Y) - H(X, Y)$
            \item $I(X; X) = H(X)$
        \end{enumerate}
    \end{thm}
\end{frame}

\begin{frame}{Theorem 2: Proof of (1)}
    \begin{proof}[Proof of (1)]
        We can use the definition of mutual information and relative entropy to obtain:
        \begin{align*}
            I(X; Y) &= D(p_{(X, Y)} \| p_X p_Y)                                                                                         && \text{(by def.~of mutual info.)}\\
                    &= \mathbb{E}_{p_{(X, Y)}}\paren{\log \frac{p_{(X, Y)}(X, Y)}{p_X(X)p_Y(Y)}}                                        && \text{(by def.~relative entropy)} \\
                    &= \mathbb{E}_{p_{(X, Y)}}\paren{\log \frac{p_X(X)p_{(Y \mid X)}{(Y \mid X)}}{p_X(X)p_Y(Y)}}                        && \text{(using cond.~probability)} \\
                    &= \mathbb{E}_{p_{(X, Y)}}\paren{\log \frac{p_{(Y \mid X)}(Y \mid X)}{p_Y(Y)}}                                      && \text{(simplify fraction)} \\
                    &= \mathbb{E}_{p_{(X, Y)}}\paren{\log p_{(Y \mid X)}(Y \mid X)} - \mathbb{E}_{p_{(X, Y)}}\paren{\log p_Y(Y)}        && \text{(simplify logarithm)} \\
                    &= -H(Y \mid X) + H(Y)                                                                                              && \text{(by def.~of entropy)}
        \end{align*}
    \end{proof}
\end{frame}

\begin{frame}{Theorem 3: Chain Rules}
    \begin{thm}[Chain Rules]
        Let $n \in \mathbb{N}, n \ge 2$, $(X_1, \cdots, X_n) \sim p(x_1, \cdots, x_n)$ and $Y$ a random variable.
        The following statements about Entropy and Mutual Information are called Chain Rules:
        \begin{enumerate}
            \item $H(X_1, \cdots, X_n) = \sum_{i=1}^{n}{H(X_i \mid X_{i-1}, \cdots, X_1)}$
            \item $D(p(x) \| q(x)) = D(p(x \mid y) \| q(x \mid y)) + D(p(y) \| q(y))$
            \item $I(X_1, \cdots, X_n; Y) = \sum_{i=1}^{n}{I(X_i; Y \mid X_{i-1}, \cdots, X_1)}$
        \end{enumerate}
    \end{thm}
\end{frame}

\begin{frame}{Theorem 3: Proof of (1)}
    \begin{proof}[Proof of (1)]
        Prove this result using induction by $n$. \\
        Base case $n = 2$: Chain rule for two variables. \\
        Assume the theorem holds for $n - 1$. Induction case $n - 1$ to $n$:
        \begin{align*}
            H(X_1, \cdots, H_n) &= H(X_n | X_1, \cdots, H_{n-1}) + H(X_1, \cdots, H_{n-1})          && \text{(apply base case)} \\
            &= H(X_n | X_1, \cdots, H_{n-1}) + \sum_{i=1}^{n-1}{H(X_i \mid X_{i-1}, \cdots, X_1)}   && \text{(induction hypothesis)}
        \end{align*}
    \end{proof}
\end{frame}

\begin{frame}{Theorem 3: Proof of (2)}
    \begin{proof}[Proof of (2)]
        \begin{align*}
            D(p(x) \| q(x)) &= \sum_{(x, y) \injoint}{p(x, y) \frac{p(x)}{q(x)}}
            = \sum_{(x, y) \injoint}{p(x, y) \frac{p(x)}{q(x)}} \\
            &= \sum_{(x, y) \injoint}{p(x, y) \frac{p(x \mid y)p(y)}{q(x \mid y)p(y)}} \\
            &= \sum_{(x, y) \injoint}{p(x, y) \frac{p(x \mid y)}{q(x \mid y)}} + \sum_{(x, y) \injoint}{p(x, y) \frac{p(y)}{p(y)}} \\
            &= D(p(x \mid y) \| q(x \mid y)) + D(p(y) \| q(y))
        \end{align*}
    \end{proof}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inequalities for Entropy and Mutual Information}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Remark 4: Convexity}
    \begin{rmk}
        We use the common definition of convex functions and concave functions from analysis.
    \end{rmk}
\end{frame}

\begin{frame}{Theorem 4: Jensen's Inequality}
    \begin{thm}[Jensen's Inequality]
        Let $\mathcal{X} \subset \mathbb{R}$ and $f: \mathcal{X} \rightarrow \mathbb{R}$ a function.
        \begin{enumerate}
            \item If $f$ is convex, we have $\mathbb{E} f(X) \ge f(\mathbb{E} X)$.
            \item If $f$ is concave, we have $\mathbb{E} f(X) \le f(\mathbb{E} X)$.
            \item If $f$ is strictly convex and $\mathbb{E} f(X) = f(\mathbb{E} X)$, we have $\mathbb{E}(X) = X$ almost surely.
        \end{enumerate}
    \end{thm}
\end{frame}

\begin{frame}{Theorem 4: Proof of (1) -- Finite case}
    \begin{proof}[Proof of (1)]
        Let $n \in \mathbb{N} \setminus \{1\}$, $p_1, \cdots, p_n \in (0, 1)$ with $\sum_{i=1}^{n}{p_i} = 1$ and $x_1, \cdots, x_n \in \mathbb{R}$.
        If $\mathcal{X}$ is finite, show $f\paren{\sum_{i=1}^{n}{p_i x_i}} \le \sum_{i=1}^{n}{p_i f(x_i)}$ by induction. \\
        Base case $i = 2$: $f(p_1 x_1 + p_2 x_2) \le p_1 f(x_1) + p_2 f(x_2)$ (def.~of convexity). \\
        Assume the claim holds for $n - 1$. Induction case:
        {\small
        \begin{align*}
            f\paren{\sum_{i=1}^{n}{p_i x_i}} &= f\paren{p_1 x_1 + (1 - p_1) \sum_{i=2}^{n}{\frac{p_i}{1 - p_1} x_i}} \\          
            &\le p_1 f\paren{x_1} + (1 - p_1) f\paren{\sum_{i=2}^{n}{\frac{p_i}{1 - p_1} x_i}}              && \text{(def. of convexity)} \\
            &\le p_1 f\paren{x_1} + (1 - p_1) \sum_{i=2}^{n}{\frac{p_i}{1 - p_1} f\paren{{x_i}}}         
            && \text{(induct. hypo.)} \\
            &= \sum_{i=1}^{n}{p_i f\paren{{x_i}}}
        \end{align*}
        }
    \end{proof}
\end{frame}

\begin{frame}{Theorem 4: Proof of (2) and (3)}
    \begin{proof}[Proof of (2)]
        Follows from part 1 applied to $-f$.
    \end{proof}
    
    \vspace{1em}
    
    \begin{proof}[Proof of (3)]
        If $f$ is strictly convex, the inequality in (1) is strict. 
        But equality holds, so all probability mass must be concentrated in one value.
    \end{proof}
\end{frame}

\begin{frame}{Corollary 1: Non-negativity (Statement)}
    \begin{cor}
        Entropy and Mutual Information are non-negative:
        \begin{enumerate}
            \item $0 \le H(X)$
            \item $0 \le D(p \| q)$
            \item $0 \le I(X; Y)$
        \end{enumerate}
    \end{cor}
\end{frame}

\begin{frame}{Corollary 1: Proof of (1)}
    \begin{proof}[Proof of (1)]
        Note that $\log(\frac{1}{[0, 1]}) = \log([1, \infty]) = [0, \infty]$ 
        and $p(X)(\mathcal{X}) \in [0, 1]$. \\
        Using the monotonicity of the expected value, we obtain
        \begin{equation*}
            0 \le \mathbb{E}\paren{\log \paren{\frac{1}{p(X)}}}
            = -\mathbb{E}\paren{\log \paren{p(X)}} = H(X)
        \end{equation*}
    \end{proof}
\end{frame}

\begin{frame}{Corollary 1: Proof of (2)}
    \begin{proof}[Proof of (2)]
        We can prove this using Jensens Inequality on a \textit{concave} function:
        \begin{align*}
            -D(p(x) \| q(x)) &= -\mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}}
            = \mathbb{E}_p\paren{\log \frac{q(X)}{p(X)}}                            && \text{(def. of relative entropy)}            \\
            &\le \log\paren{\mathbb{E}_p \frac{q(X)}{p(X)}}                         && \text{(Jensen's inequality)} \\
            &= \log\paren{\sum_{x \in \mathcal{X}}{p(x) \frac{q(x)}{p(x)}}}
            = \log\paren{\sum_{x \in \mathcal{X}}{q(x)}}                            && \text{(def. of exp. value, simplify)}  \\
            &= \log\paren{1} = 0                                                    && \text{(q is a prob. function)}
        \end{align*}
        So equivalently, we have $D(p(x) \| q(x)) \ge 0$.
    \end{proof}
\end{frame}

\begin{frame}{Corollary 1: Proof of (3)}
    \begin{proof}[Proof of (3)]
        Follows from part (2): $I(X; Y) = D(p(x, y) \| p(x)p(y)) \ge 0$.
    \end{proof}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Advanced Properties of Entropy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Remark 5: Natural Questions about Entropy}
    \begin{rmk}
        There are multiple natural questions we can ask about Entropy.
        We will look at an example for each of them and then prove the results.
        \begin{enumerate}
            \item Can conditioning on more information increase the entropy?
            \item What finite distribution minimises and what distribution maximises the value of Entropy?
            \item Can joint entropy increase if we add redundant information?
            \item What happens to the Entropy if we add independent noise to our measurements?
        \end{enumerate}
    \end{rmk}
\end{frame}

%% Question 1: Information never hurts
\begin{frame}{Remark 5: Question 1 -- Can conditioning increase entropy?}
    \begin{rmk}
        \textbf{Question 1:} Can conditioning on more information increase the entropy?
        
        \vspace{0.5em}
        The Venn diagram illustrates that conditioning on more information can never increase the entropy.
    \end{rmk}
    \begin{thm}
        More information can only \textit{decrease} entropy: $H(X \mid Y) \le H(X)$.
    \end{thm}
\end{frame}

\begin{frame}{Theorem 5: Proof of (1) -- Information never hurts}
    \begin{proof}
        $0 \le I(X; Y) = H(X) - H(X \mid Y) \iff H(X \mid Y) \le H(X)$
    \end{proof}
\end{frame}

%% Question 2: Distributions that extremize entropy
\begin{frame}{Remark 5: Question 2 -- What distributions extremize entropy?}
    \begin{rmk}
        \textbf{Question 2:} What finite distribution minimises and what distribution maximises the value of Entropy?
        
        \vspace{0.5em}
        Let $X_p \sim B(p_1, \cdots, p_n)$ be a categorical distribution.
        We can write the question as an optimisation problem: $\min_{p \in \mathbb{R}^{n}, \|p\|_1=1, p \ge 0}{H(X_p)}$.
        
        \vspace{0.5em}
        From the figures, it seems plausible that the uniform distribution maximises entropy and a peaked distribution minimises entropy.
    \end{rmk}
    \begin{thm}
        \begin{itemize}
            \item A peaked distribution minimises entropy: $H(X) = 0 \iff \exists x \in \mathcal{X}: p_X(x) = 1$.
            \item The uniform distribution maximizes entropy: $H(X) = \log |\mathcal{X}| \iff X \sim U(\mathcal{X})$.
        \end{itemize}
    \end{thm}
\end{frame}

\begin{frame}{Remark 5: Figures for Question 2}
    \begin{figure}
        \centering
        \begin{minipage}{0.45\textwidth}
            \centering
            \includegraphics[width=\linewidth]{../plots/peak_distribution_minimises_entropy.pdf}
            \captionof{figure}{Peak Distribution minimises Entropy}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \includegraphics[width=\linewidth]{../plots/uniform_distribution_maximises_entropy.pdf}
            \captionof{figure}{Uniform Distribution maximises Entropy}
        \end{minipage}
    \end{figure}
\end{frame}

\begin{frame}{Theorem 5: Proof of (2a) -- Peaked distribution minimises entropy}
    \begin{proof}
        Let $X$ be a discrete random variable with an entropy. Then we have
        \begin{align*}
            H(X) = 0
            &\iff \forall x \in \mathcal{X}: p_X(x) \log p_X(x) = 0 \\
            &\iff \forall x \in \mathcal{X}: p_X(x) = 0 \oplus p_X(x) = 1
        \end{align*}
    \end{proof}
\end{frame}

\begin{frame}{Theorem 5: Proof of (2b) -- Uniform distribution maximises entropy}
    \begin{proof}
        Let $Y \sim U(\mathcal{X})$ st. $\forall x \in \mathcal{X}\vcentcolon q(x)=\frac{1}{|\mathcal{X}|}$.
        \begin{align*}
            0 &\le D(p \| q)
            = \mathbb{E}_p\paren{\log \frac{p(X)}{q(X)}}
            = \sum_{x \in \mathcal{X}}{p(x) \log \frac{p(x)}{q(x)}} \\
            &= \sum_{x \in \mathcal{X}}{p(x) \log\paren{p(x)|\mathcal{X}|}}
            = \sum_{x \in \mathcal{X}}{p(x) \log p(x)} + \log |\mathcal{X}| \sum_{x \in \mathcal{X}}{p(x)}\\
            &= -H(X) + \log |\mathcal{X}| = \log |\mathcal{X}| - H(X)
        \end{align*}
        This is equivalent to $H(X) \le \log |\mathcal{X}|$.
        Lastly, Jensen's Inequality (3) yields the equivalence.
    \end{proof}
\end{frame}

%% Question 3: Redundant information
\begin{frame}{Remark 5: Question 3 -- Can redundant info increase entropy?}
    \begin{rmk}
        \textbf{Question 3:} Can joint entropy increase if we add redundant information?
        
        \vspace{0.5em}
        Let $\Omega = \{1, 2\}$, $\mathds{P}$ uniform, $X, Y: \Omega \rightarrow \mathbb{R}$, $X(\omega) = \omega$ and $Y(\omega) = 2\omega$.
        $Y$ is redundant to $X$, as $Y = 2X$.
        We can calculate $H(X) = H(X, Y) = 1$.
        This suggests entropy never increases with redundant info.
    \end{rmk}
    \begin{thm}
        If information from $Y$ does not add anything to $X$, then $Y$ must be derived from $X$: \\
        $H(Y \mid X) = 0 \implies \exists f\vcentcolon Y = f(X) \; \text{almost surely}$
    \end{thm}
\end{frame}

\begin{frame}{Theorem 5: Proof of (3) -- Redundant information}
    \begin{proof}
        We have $H(Y \mid X) = -\mathbb{E}\paren{\log p(Y \mid X)} = \sum_{(x, y) \injoint}{-p(x, y) \log p(y \mid x)}$. \\
        Additionally, $\forall (x, y) \in \mathcal{X} \times \mathcal{Y}: -p(x, y) \log p(y \mid x) \ge 0$. \\
        Combining:
        \begin{align*}
            H(Y \mid X) = 0
            &\iff \forall (x, y) \injoint: p(x, y) \log p(y \mid x) = 0 \\
            &\iff \forall (x, y) \injoint: p(x, y) = 0 \oplus p(y \mid x) = 1
        \end{align*}
        This tells us, that either $p(x, y) = 0$ or $p(x, y) = p(y \mid x)p(x) = p(x) = 1$. \\
        Define $(y_x)_{x \in \mathcal{X}}$ such that $\forall x \in \mathcal{X}: p(x, y_x) > 0$. \\
        Set $f: \supp(X) \rightarrow \mathcal{Y}, x \mapsto y_x$. This gets us $Im(f) = \{y_x: x \in \mathcal{X}\} = \{y \in \mathcal{Y}: p(x, y) > 0\}$. \\
        So $Y = f(X) \; \text{almost surely}$.
    \end{proof}
\end{frame}

%% Question 4: Independent noise
\begin{frame}{Remark 5: Question 4 -- What happens with independent noise?}
    \begin{rmk}
        \textbf{Question 4:} What happens to the Entropy if we add independent noise to our measurements?
        
        \vspace{0.5em}
        Let $X \sim U(\{1, 2, 3, 4\})$ be the original signal, $N \sim U(\{0, 1\})$ the noise and let $X, N$ be independent. 
        Then $S \defeq X + N$ is a noisy signal. We expect the entropy to never decrease.
    \end{rmk}
    \begin{thm}
        If independent noise is added to a random variable, entropy can only increase: \\
        Set $Z = X + Y$. Then: $X, Y \text{ independent} \implies H(X) \le H(Z) \land H(Y) \le H(Z)$
    \end{thm}
\end{frame}

\begin{frame}{Theorem 5: Proof of (4) -- Independent noise}
    \begin{proof}
        We have
        \begin{align*}
             H(Z \mid X) &= -\mathbb{E}\paren{\log p(Z \mid X)} = \sum_{(x, y) \injoint}{-p(x, y) \log P[Z = x + y \mid X = x]} \\
             &= \sum_{(x, y) \injoint}{-p(x, y) \log P[X + Y = x + y \mid X = x]} \\
             &= \sum_{(x, y) \injoint}{-p(x, y) \log P[Y = y \mid X = x]}
             = -\mathbb{E}\paren{\log p(Y \mid X)} = H(Y \mid X)
        \end{align*}
        Using Independence and Information never hurts, we get
        $H(X) = H(X \mid Y) = H(Z \mid Y) \le H(Z)$
        and
        $H(Y) = H(Y \mid X) = H(Z \mid X) \le H(Z)$.
    \end{proof}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Log-Sum Inequality and Convexity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Theorem 6: Log-Sum Inequality}
    \begin{thm}[Log-Sum Inequality]
        Let $n \in \mathbb{N}$, $a_1, \cdots, a_n, b_1, \cdots, b_n \ge 0$.
        Then we have 
        \begin{align*}
            \sum_{i=1}^{n}{a_i \log\frac{a_i}{b_i}} 
            \ge \paren{\sum_{i=1}^{n}{a_i}}\log\frac{\sum_{i=1}^{n}{a_i}}{\sum_{i=1}^{n}{b_i}}
        \end{align*}
    \end{thm}
\end{frame}

\begin{frame}{Theorem 7: Convexity of Relative Entropy (Statement)}
    \begin{thm}[Convexity of Entropy]
        \begin{enumerate}
            \item $D(. \| .)$ is a convex function. This means that
            \begin{align*}
                \forall p_1, q_1, p_2, q_2, \forall \lambda \in [0, 1]\vcentcolon \quad
                &D(\lambda p_1 + (1-\lambda) p_2 \| \lambda q_1 + (1-\lambda) q_2) \\
                &\le \lambda D(p_1 \| q_1) + (1 - \lambda) D(p_2 \| q_2)
            \end{align*}
            
            \item Let $X_p \sim B(1, p)$ for all $p \in (0, 1)$ and $h(p) \defeq H(X_p)$. \\
            Then $h \le 1$ and $h$ is a concave function.
        \end{enumerate}
    \end{thm}
\end{frame}

\begin{frame}{Theorem 7: Proof of (1)}
    \begin{proof}
        Let $p_1, q_1, p_2, q_2$ be probability mass functions on $\mathcal{X}$.
        Let $\lambda \in (0, 1)$. \\
        First of all, the set of probability densities is convex. 
        So the above statement is well-defined. \\
        Secondly, the inequality needs to be verified:
        \begin{align*}
            &D(\lambda p_1 + (1-\lambda) p_2 \| \lambda q_1 + (1-\lambda) q_2) \\
            &= \sum_{x \in \mathcal{X}}{\paren{\lambda p_1(x) + (1-\lambda) p_2(x)} \log \frac{\lambda p_1(x) + (1-\lambda) p_2(x)}{\lambda q_1(x) + (1-\lambda) q_2(x)}}      
            && \text{(by definition)} \\
            &\le \sum_{x \in \mathcal{X}}{\lambda p_1(x) \log \frac{p_1(x)}{q_1(x)} + (1-\lambda) p_2(x) \log \frac{p_2(x)}{q_2(x)}}        && \text{(log-sum inequality)} \\
            &= \lambda D(p_1 \| q_1) + (1-\lambda) D(p_2 \| q_2)                                                                            && \text{(by definition)}
        \end{align*}
    \end{proof}
\end{frame}

\begin{frame}{Theorem 7: Proof of (2) -- $h \le 1$}
    \begin{proof}
        First, we have
        \begin{align*}
            h(p) &= H(X_p) = -p \log p - (1-p) \log (1-p)       && \text{(by definition)} \\
            &= -(p \log p + (1-p) \log (1-p))                   && \text{(factor out)} \\
            &\le -(p + 1 - p) \log \frac{p + 1 - p}{1 + 1} = -\log \frac{1}{2} = 1  && \text{(simplify)}
        \end{align*}
    \end{proof}
\end{frame}

\begin{frame}{Theorem 7: Proof of (2) -- $h$ is concave}
    \begin{proof}
        For $q$ with $\forall x \in \mathcal{X}: q(x)=\frac{1}{|\mathcal{X}|}$.
        Let $p_1, p_2 \in [0, 1]$ and $\lambda \in (0, 1)$.
        {\small
        \begin{align*}
            h(\lambda p_1 + (1-\lambda) p_2) &= H(X_{\lambda p_1 + (1-\lambda) p_2})                            && \text{(by definition)} \\
            &= |\mathcal{X}| - D(\lambda p_1 + (1-\lambda) p_2 \| q)                                            && \text{(using Theorem 5)} \\
            &\ge |\mathcal{X}| - \lambda D(p_1 \| q) - (1-\lambda) D(p_2 \| q)                                  && \text{(using convexity)} \\
            &= \lambda \paren{|\mathcal{X}| - D(p_1 \| q)} + (1-\lambda) \paren{|\mathcal{X}| - D(p_2 \| q)}    && \text{(factor out twice)} \\
            &= \lambda H(X_{p_1}) + (1-\lambda) H(X_{p_2}) = \lambda h(p_1) + (1-\lambda) h(p_2)                && \text{(by definition)}
        \end{align*}
        }
    \end{proof}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application to Optimisation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Example 5: MNIST Digit Classification}
    \begin{exmp}[MNIST Digit Classification]
        We can now apply the concept of Relative Entropy to solve a common classification problem from machine learning.
        Let $\Omega$ be the set of $28$x$28$ pixel images that contain exactly one handwritten digit from $0$ to $9$.
        The task is to predict the digit $0$ to $9$, based on the input image $\omega \in \Omega$.
        
        \vspace{0.5em}
        In order to accomplish this task, we define a model $f: \mathbb{R}^n \rightarrow \mathbb{R}_{+}^{10}$ with $n \in \mathbb{N}$.
        This function $f$ takes parameters as inputs that allow it to output a probability mass function that indicates the digit in the image.
        So $\forall \omega \in \Omega\vcentcolon f(\omega) \ge 0 \land \sum_{i=1}^{10} f(\omega)_i = 1$.
    \end{exmp}
\end{frame}

\begin{frame}{Example 5: MNIST Digit Classification (1/2)}
    The optimisation objective is
    \begin{align*}
        \Phi = \argmin_{\phi \in \mathbb{R}^n}{D(d \| f_{\phi})}
    \end{align*}
    As we have shown in Theorem 7, the Relative Entropy is a convex function.
    
    \vspace{0.5em}
    For $f$, we can use a two-layer convolutional neural network with dropout.
    In this case, the model $f$ is not a convex function itself. So the optimisation objective is not convex either.
    But at least the loss function $D(d \| .)$ is convex.
    We can now use Stochastic Gradient Descent to optimise for $60$ steps with step size 5e-3 and batch size $512$.
\end{frame}

\begin{frame}{Example 5: MNIST Digit Classification (2/2)}
    \begin{figure}
        \centering
        \begin{minipage}{0.45\textwidth}
            \centering
            \includegraphics[width=0.9\linewidth]{../plots/mnist_loss.pdf}
            \captionof{figure}{MNIST objective function}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \includegraphics[width=0.9\linewidth]{../plots/mnist_predictions.pdf}
            \captionof{figure}{MNIST images and pred. digit distr.}
        \end{minipage}
    \end{figure}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Summary}
    \begin{itemize}
        \item Defined \textbf{Entropy}, \textbf{Mutual Information}, and \textbf{Relative Entropy}
        \item Established \textbf{Chain Rules} for entropy and mutual information
        \item Proved \textbf{Jensen's Inequality} and non-negativity of entropy measures
        \item Showed which distributions \textbf{extremize entropy}
        \item Proved the \textbf{Log-Sum Inequality} and convexity of relative entropy
        \item Connected theory to practice via \textbf{Relative Entropy} loss in neural networks
    \end{itemize}
\end{frame}

\begin{frame}{References}
    \begin{thebibliography}{99}
        \bibitem{cover-thomas} T.~Cover and J.~Thomas, \emph{Elements of Information Theory}, Wiley, 2006.
    \end{thebibliography}
\end{frame}

\begin{frame}
    \centering
    \Huge Thank you!
    
    \vspace{1em}
    \Large Questions?
\end{frame}

\end{document}
